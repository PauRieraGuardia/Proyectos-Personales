{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["heF773I_awZe","adVZ0Wrzcsdw"],"authorship_tag":"ABX9TyM4aAS2m0+dmYbJn77edZMu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MACHINE LEARNING\n","\n","El Machine Learning (ML) es una rama de la inteligencia artificail que permite a los sistemas aprender automáticamente a partir de los datos, sin ser explícitamente programador para cada tarea. A diferencia de los métodos tradicionales, el ML se basa en extraer patrones y relaciones entre las variables de entrada (*features*) y las de salida (*target*), para luego <b>predecir, clasificar o estimar</b> nuevos valores.\n","\n","Su objetivo principal es desarrollar modelos que sean capaces de <b>generalizar</b>, es decir, que funcionen bien no solo con los datos de entrenamiento, sino también con datos nuevos y desconocidos."],"metadata":{"id":"rIBasuiThcKy"}},{"cell_type":"markdown","source":["#### <b>Tipos de aprendizaje en Machine Learning</b>\n","\n","El Machine Learning se divide en tres grandes tipos de aprendizaje, en función del tipo de datos disponibles y del objetivo del modelo:\n","\n","1. Aprendizaje supervisado\n","\n","    * Se dispone de un conjunto de datos con etiquetas conocidas (target).\n","\n","    * El modelo aprende a predecir una salida a partir de ejemplos ya etiquetados.\n","\n","    * Ejemplos:\n","\n","        * Regresión: predecir un valor continuo (precio, temperatura, beneficio).\n","\n","        * Clasificación: predecir una categoría (fraude/no fraude, sí/no, tipo de cliente).\n","\n","\n","2. <b>Aprendizaje no supervisado\n","\n","    * No existen etiquetas conocidas.\n","\n","    * El modelo busca patrones ocultos o agrupaciones naturales en los datos.\n","\n","    * Ejemplos:\n","\n","        * Clustering: agrupar clientes por comportamiento (K-Means, DBSCAN).\n","\n","        * Reducción de dimensionalidad: simplificar datos (PCA). </b>\n","\n","3. Aprendizaje por refuerzo\n","\n","* Un agente aprende por recompensas o penalizaciones, tomando decisiones secuenciales.\n","\n","* Menos usado en este módulo, más presente en IA avanzada (robótica, juegos, control automático)."],"metadata":{"id":"ZRRuTCM-hfZL"}},{"cell_type":"markdown","source":["## 1. Introducción al Aprendizaje NO supervisado\n","\n","El aprendizaje no supervisado abarca técnicas curo objetivo es descubir patrones ocultos en los datos sin conocer etiquetas (target).\n","\n","A direcenia del aprendizaje supervisado, aquí el modelo no predice nada: explora, organizar, resumo o transforma la estructura interna del dataset.\n","\n","Sus objetivos principales incluyen;\n","\n","* Reducir dimensionalidad para simplificar los datos manteniendo su información relevante.\n","* Agrupar observaciones similares (clustering)\n","* Explorar relaciones, patrones y estructuras no evidentes.\n","* Preparar features más compactas y útiles para modelos posteriores,\n","\n","En este módulo veremos dos métodos fundamentales:\n","\n","1. PCA - Reducción de dimensionalidad\n","2. K-Means - Clustering"],"metadata":{"id":"elMWnzXtXTQo"}},{"cell_type":"markdown","source":["## 2. Reducción de Dimensionalidad: PCA"],"metadata":{"id":"heF773I_awZe"}},{"cell_type":"markdown","source":["### 2.1 ¿Qué es PCA?\n","\n","El Análisis de Componentes Principales (PCA) es una técnica que transforma las variables originales en un nuevo conjunto de variables llamadas componentes principales, que:\n","\n","* Son combinaciones lineales de las variables originales.\n","\n","* Están ordenadas según la cantidad de varianza que explican.\n","\n","* Permiten representar los datos en un espacio reducido, perdiendo la menor información posible."],"metadata":{"id":"tJztUL5tazlJ"}},{"cell_type":"markdown","source":["### 2.2 Objetivos del PCA\n","* Reducir dimensiones para simplificar modelos y acelerar cálculos.\n","\n","* Visualizar datos en 2D o 3D.\n","\n","* Filtrar ruido manteniendo solo la información relevante.\n","\n","* Eliminar correlaciones entre variables.\n","\n","* Mejorar modelos supervisados generando features más compactas."],"metadata":{"id":"EJiRlgdLa8au"}},{"cell_type":"markdown","source":["### 2.3 Flujo general del PCA\n","\n","1. Estándar los datos (si tienen escalas distintas).\n","\n","2. Ajustar el modelo PCA especificando cuántas componentes queremos.\n","\n","3. Transformar los datos al nuevo espacio reducido.\n","\n","4. (Opcional) Reconstruir para evaluar la pérdida de información.\n","\n","5. Analizar la varianza explicada y elegir el número óptimo de componentes."],"metadata":{"id":"piOw_xmta-ke"}},{"cell_type":"markdown","source":["### 2.4 Código base para PCA\n","\n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# 1. Seleccionar las variables numéricas\n","X = df.select_dtypes(include=['int64', 'float64'])\n","\n","# 2. Estandarizar (recomendado)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# 3. Ajustar PCA (ejemplo: 2 componentes)\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# 4. DataFrame con componentes principales\n","df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n","df_pca.head()\n","```"],"metadata":{"id":"786LM-r8bo0p"}},{"cell_type":"markdown","source":["### 2.5 Varianza explicada y selección del número de componentes\n","\n","```python\n","pca_full = PCA().fit(X_scaled)\n","\n","var_explicada = pca_full.explained_variance_ratio_\n","var_acum = pca_full.explained_variance_ratio_.cumsum()\n","\n","pd.DataFrame({\n","    'Componente': range(1, len(var_explicada) + 1),\n","    'Varianza_explicada': var_explicada,\n","    'Varianza_acumulada': var_acum\n","})\n","```\n","\n","<b>Interpretación:</b>\n","\n","Elegimos `n_components` donde la varianza acumulada alcance un umbral (por ejemplo, 90% o 95%)."],"metadata":{"id":"z_XugXNFcGst"}},{"cell_type":"markdown","source":["### 2.6 Reconstrucción de los datos (PCA como filtro de ruido)\n","\n","```python\n","# Ajustar PCA con más componentes\n","pca_filter = PCA(n_components=10)\n","X_reduced = pca_filter.fit_transform(X_scaled)\n","\n","# Reconstrucción (aproximación del original)\n","X_reconstructed = pca_filter.inverse_transform(X_reduced)\n","```\n","\n","<b>Interpretación:</b>\n","\n","La reconstrucción pierde detalles de alta frecuencia → permite eliminar ruido."],"metadata":{"id":"Kr5KzkawcR_V"}},{"cell_type":"markdown","source":["## 3. Clustering: K-Means"],"metadata":{"id":"adVZ0Wrzcsdw"}},{"cell_type":"markdown","source":["### 3.1 ¿Qué es K-Means?\n","\n","K-Means es un algoritmo que agrupa los datos en k clústeres según su similaridad.\n","Funciona encontrando centroides que minimizan la distancia a los puntos asignados a cada grupo."],"metadata":{"id":"wWNg22-mdTYV"}},{"cell_type":"markdown","source":["### 3.2 Objetivos de K-Means\n","\n","* Descubrir grupos naturales en los datos.\n","\n","* Analizar comportamiento o estructura interna.\n","\n","* Segmentar clientes, productos u observaciones similares.\n","\n","* Crear features adicionales para modelos supervisados (cluster membership).\n","\n","* Realizar compresión en imágenes o señales."],"metadata":{"id":"iVM9U6aWdUky"}},{"cell_type":"markdown","source":["### 3.3 Flujo general del K-Means\n","\n","Seleccionar número inicial de clústeres k.\n","\n","Ajustar el modelo con fit_predict.\n","\n","Obtener etiquetas de clúster y centroides.\n","\n","Evaluar mediante inertia o silhouette score.\n","\n","Analizar resultados (segmentación, interpretación de grupos)."],"metadata":{"id":"4xAZZx-DdWyo"}},{"cell_type":"markdown","source":["### 3.4 Código base de K-Means\n","\n","```python\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","\n","# 1. Seleccionar variables numéricas\n","X = df.select_dtypes(include=['int64', 'float64'])\n","\n","# 2. Escalado (importante para clustering)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# 3. Ajustar modelo K-Means\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","labels = kmeans.fit_predict(X_scaled)\n","\n","# 4. Añadir los clusters al DataFrame original\n","df['cluster'] = labels\n","df.head()\n","```"],"metadata":{"id":"5xukr0Khd3Br"}},{"cell_type":"markdown","source":["### 3.5 Determinar el valor óptimo de k (Elbow Method)\n","\n","```python\n","inertias = []\n","Ks = range(2, 11)\n","\n","for k in Ks:\n","    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n","    inertias.append(km.inertia_)\n","\n","pd.DataFrame({\n","    'k': Ks,\n","    'inercia': inertias\n","})\n","```\n","\n","<b>Interpretación:</b>\n","\n","Buscamos el “codo”, donde la reducción de inercia empieza a ser marginal.\n"],"metadata":{"id":"NEvFvUl0eAiB"}},{"cell_type":"markdown","source":["### 3.6 Evaluación del clustering (Silhouette Score)\n","\n","```python\n","from sklearn.metrics import silhouette_score\n","\n","score = silhouette_score(X_scaled, labels)\n","print(\"Silhouette Score:\", score)\n","```\n","<b>Interpretación:</b>\n","* Valores cercanos a 1 → clústeres bien separados.\n","* Valores cercanos a 0 → clústeres muy mezclados.\n","* Valores negativos → asignación incorrecta."],"metadata":{"id":"6m8DzfF7ech4"}},{"cell_type":"markdown","source":["### 3.7 Centroides del modelo\n","\n","```python\n","centroides = kmeans.cluster_centers_\n","centroides_df = pd.DataFrame(centroides, columns=X.columns)\n","centroides_df\n","```\n","\n","<b>Interpretación:</b>\n","\n","Cada centroide representa el “perfil medio” de ese clúster."],"metadata":{"id":"1syMBVN6fRMl"}},{"cell_type":"markdown","source":["## 4. Integración PCA + K-Means\n","\n","En datasets con muchas variables:\n","\n","* Primero PCA → reduce dimensionalidad y elimina ruido.\n","* Después K-Means → agrupa mejor al trabajar en un espacio más limpio.\n","\n","Flujo genérico combinado\n","\n","```python\n","# 1. Variables numéricas\n","X = df.select_dtypes(include=['int64', 'float64'])\n","\n","# 2. Escalado\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# 3. PCA (elegir número de componentes)\n","pca = PCA(n_components=5)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# 4. K-Means sobre el espacio reducido\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","labels = kmeans.fit_predict(X_pca)\n","\n","df['cluster'] = labels\n","```\n","\n","<b> Ventajas: </b>\n","* Menor ruido.\n","* Clústeres más estables.\n","* Menor coste computacional."],"metadata":{"id":"e8YRC3hJfgcZ"}}]}
