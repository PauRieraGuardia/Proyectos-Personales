{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["dSN3p4-IhQ3X","TD2onWYYmoxU","XuD5gn5_ra3t","MJDINDTTsr1y","2z7RHXRdux7G","kvDxYkctwfcL","EY5DTYVoyBuS","LwdkceK-yhC8","O_chESh63Mhb","1M5qECr4819U","WovWD3Zz_HJG","ChDSIENiAwhz"],"authorship_tag":"ABX9TyP1uH6bmcW+D4a2FqXst3Wk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MACHINE LEARNING\n","\n","El Machine Learning (ML) es una rama de la inteligencia artificail que permite a los sistemas aprender automáticamente a partir de los datos, sin ser explícitamente programador para cada tarea. A diferencia de los métodos tradicionales, el ML se basa en extraer patrones y relaciones entre las variables de entrada (*features*) y las de salida (*target*), para luego <b>predecir, clasificar o estimar</b> nuevos valores.\n","\n","Su objetivo principal es desarrollar modelos que sean capaces de <b>generalizar</b>, es decir, que funcionen bien no solo con los datos de entrenamiento, sino también con datos nuevos y desconocidos."],"metadata":{"id":"6M_33afD6n0I"}},{"cell_type":"markdown","source":["#### <b>Tipos de aprendizaje en Machine Learning</b>\n","\n","El Machine Learning se divide en tres grandes tipos de aprendizaje, en función del tipo de datos disponibles y del objetivo del modelo:\n","\n","1. <b>Aprendizaje supervisado\n","\n","    * Se dispone de un conjunto de datos con etiquetas conocidas (target).\n","\n","    * El modelo aprende a predecir una salida a partir de ejemplos ya etiquetados.\n","\n","    * Ejemplos:\n","\n","        * Regresión: predecir un valor continuo (precio, temperatura, beneficio).\n","\n","        * Clasificación: predecir una categoría (fraude/no fraude, sí/no, tipo de cliente). </b>\n","\n","\n","2. Aprendizaje no supervisado\n","\n","    * No existen etiquetas conocidas.\n","\n","    * El modelo busca patrones ocultos o agrupaciones naturales en los datos.\n","\n","    * Ejemplos:\n","\n","        * Clustering: agrupar clientes por comportamiento (K-Means, DBSCAN).\n","\n","        * Reducción de dimensionalidad: simplificar datos (PCA).\n","\n","3. Aprendizaje por refuerzo\n","\n","* Un agente aprende por recompensas o penalizaciones, tomando decisiones secuenciales.\n","\n","* Menos usado en este módulo, más presente en IA avanzada (robótica, juegos, control automático)."],"metadata":{"id":"f7w3brbo8xwd"}},{"cell_type":"markdown","source":["#### <b>Etapas generales del proceso de Machine Learning en modelos supervisados</b>\n","\n","Todo proyecto de Machine Learning sigue un flujo común:\n","\n","1. *Definición del problema* → identificar si se trata de regresión, clasificación o agrupamiento.\n","2. *Análisis exploratorio de datos (EDA)*→ conocer las características de los datos.\n","3. *Preprocesamiento*→ limpiar, transformar y preparar los datos para el modelo.\n","4. *Construcción y entrenamiento del modelo* → aplicar algoritmos de ML a los datos.\n","5. *Evaluación* → medir el rendimiento con métricas adecuadas.\n","6. *Optimización y mejora* → ajustar hiperparámetros, aplicar regularización y validar el modelo.\n","7. *Implementación y predicción final* → utilizar el modelo con nuevos datos."],"metadata":{"id":"jqVjpSSK8zPE"}},{"cell_type":"markdown","source":["#### <b>Librerías Necesarias</b>\n","\n","Para la implementación práctica de los modelos de Machine Learning, utilizamos principalmente librerías de Python, cada una enfocada en una etapa específica del flujo de trabajo\n","\n","```python\n","# Manipulación de datos\n","import pandas as pd\n","import numpy as np\n","\n","# Visualización\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Preprocesamiento y modelado\n","from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, mean_squared_error, r2_score\n","\n","# Modelos de Machine Learning\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n","\n","# Control de warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","```"],"metadata":{"id":"4BW6qOz293BO"}},{"cell_type":"markdown","source":["#### <b>Conceptos fundamentales </b>\n","\n","Antes de empezar con la práctica, es importante tener claros algunos conceptos comunes en cualquier modelo de Machine Learning:\n","\n","* Feature o variable independiente: cada columna del dataset que se utiliza como entrada.\n","\n","* Target o variable dependiente: columna que queremos predecir.\n","\n","* Entrenamiento: proceso por el cual el modelo aprende patrones de los datos.\n","\n","* Validación: fase para ajustar y comparar modelos sin usar los datos de prueba.\n","\n","* Prueba (Test): evaluación final con datos nunca vistos.\n","\n","* Overfitting: cuando el modelo aprende demasiado los datos de entrenamiento y generaliza mal.\n","\n","* Underfitting: cuando el modelo no es capaz de aprender lo suficiente (demasiado simple).\n"],"metadata":{"id":"R3kEsqkX-CcI"}},{"cell_type":"markdown","source":["## 1. Análisis exploratorio de Datos (EDA)\n","\n","El Análisis Exploratorio de Datos es la primera fase de cualquier proyecto de Machine Learning. Su propósito es comprender la estructura, distribución y calidad de los datos, así como identificar patrones, errores o relaciones que guiarán las decisiones posteriores de preprocesamiento y modelado.\n","\n","El EDA permite obtener una visión global del dataset y responder preguntas clabe:\n","\n","* ¿Qué información contiene el dataset?\n","* ¿Qué tipo de variables existen (numéricas, categóricas, temporales)?\n","* ¿Existen valores faltantes o duplicados?\n","* ¿Qué relaciones hay entre las variables?\n","* ¿Cómo se distribuye la variable objetivo (target)?\n","\n","Un EDA riguroso es esencial para evitar errores más adelante, ya que la calidad del modelo nunca será mejor que la calidad de los datos con los que se entrena."],"metadata":{"id":"5AEPMggirEGY"}},{"cell_type":"markdown","source":["### 1.1. Carga y visión general del dataset\n","\n","<b>Objetivo:</b> Obtener una primera fotografia del dataset, su estructura general, tamaño, tipos de variables y variable objetivo (target).\n","\n","Esta etapa busca familiarizarnos con los datos antes de manipularlos. Es habitual realizar comprobaciones básicas sobre:\n","\n","* El número observaciones y columnas\n","* El tipo de variables (numéricas, categóricas, temporales).\n","* La existencia o no de la variable objetivo (target).\n","* La coherencia general de los nombres de las variables.\n","\n","```python\n","# Carga del dataset (ajustar ruta según el entorno)\n","df = pd.read_csv(\"ruta/dataset.csv\")\n","\n","# Dimensiones del dataset: número de filas y columnas\n","print(\"Dimensiones del dataset:\", df.shape)\n","\n","# Visualización de las primeras observaciones\n","df.head()\n","```\n","\n","```python\n","# Información básica del dataset\n","df.info()\n","\n","# Identificación de tipos de variables\n","df.dtypes.value_counts()\n","```\n","\n","```python\n","# Identificación del target y separación de variables predictoras\n","target_col = \"target\"  # Sustituir por el nombre real del target\n","features = [col for col in df.columns if col != target_col]\n","\n","# Clasificación de variables por tipo\n","num_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n","cat_features = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n","\n","print(\"Número de variables numéricas:\", len(num_features))\n","print(\"Número de variables categóricas:\", len(cat_features))\n","```\n","\n","<b>Interpretación:</b>\n","\n","En este punto tenemos una visión global del dataset: Cúantas variables lo componen, qué tipos de datos contiene y cuál es la variable objetivo.\n","\n","Estos primeros pasos permiten detectar si el dataset está completo o requiere limpieza estructural antes de avanzar."],"metadata":{"id":"e87VhaTMww-5"}},{"cell_type":"markdown","source":["### 1.2. Estadísticas descriptivas\n","\n","<b>Objetivo:</b> Analizar el comportamiento individual de las variables, tanto numéricas como categóricas, para entender su rango, dispersión, asimetría y valores característicos.\n","\n","Las variables numéricas permiten calcular medidas de tendencia central (media, mediana) y dispersión (desviación estándar, rango).\n","\n","Las variables categóricas se analizan en función de la frecuencia de aparición de sus categorías.\n","\n","Este análisis proporciona una primera idea sobre:\n","\n","* Posibles valores anómalos o fuera de rango.\n","* Distribuciones muy sesgadas o concentradas.\n","* Categorías con escasa representación (que podrían reagruparse).\n","\n","```python\n","# Estadísticas descriptivas para variables numéricas\n","df[num_features].describe()\n","\n","# Estadísticas descriptivas para variables categóricas\n","df[cat_features].describe()\n","\n","# Distribución de frecuencias para variables categóricas\n","for col in cat_features:\n","    print(f\"\\nDistribución de {col}:\")\n","    print(df[col].value_counts(normalize=True).head())\n","```\n","\n","<b> Interpretacin: </b>\n","\n","* Variables numéricas con rangos muy amplios o desviaciones altas podrían requerir escalado o normalización.\n","* Variables categóricas con muchas categorías poco representadas podrían reagruparse (fase de codificación).\n","* Este análisis también ayuda a detectar posibles errores de codificación, como valores \"\"? o \"unkown\"."],"metadata":{"id":"XAcOCnsrDInx"}},{"cell_type":"markdown","source":["### 1.3. Valores faltantes, duplicados y outliers\n","\n","<b>Objetivo</b> Evaluar la calidad del dataset identificando valores faltantes, registros duplicados y valores atipicos (outliers).\n","\n","* Valores faltantes (NaN): indican ausencia de información. Si su proporción es alta, pueden comprometer el entrenamiento.\n","* Duplicados: Filas repetidas que deben eliminarse para evitar sesgos.\n","* Outliers: observaciones extremas que pueden distorsionar el modelo si no se tratan adecuadamente.\n","\n","En esta etapa únicamente identificamos estos problemas, su tratamiento se definirá más adelante en el preprocesamiento o la regularización.\n","\n","```python\n","# Cálculo del porcentaje de valores faltantes por variable\n","missing_count = df.isnull().sum()\n","missing_pct = (missing_count / len(df)) * 100\n","\n","missing_df = pd.DataFrame({\n","    \"missing_count\": missing_count,\n","    \"missing_pct\": missing_pct\n","}).sort_values(\"missing_pct\", ascending=False)\n","\n","print(\"Valores faltantes por variable:\")\n","display(missing_df[missing_df[\"missing_count\"] > 0])\n","```\n","\n","```python\n","# Detección de duplicados\n","num_duplicated = df.duplicated().sum()\n","print(f\"Número de filas duplicadas: {num_duplicated}\")\n","```\n","\n","```python\n","# Detección visual de outliers con boxplot\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","for col in num_features:\n","    plt.figure(figsize=(6, 3))\n","    sns.boxplot(x=df[col])\n","    plt.title(f\"Distribución y detección visual de outliers en '{col}'}\")\n","    plt.show()\n","```\n","\n","```python\n","# Detección numérica de outliers mediante rango intercuartílico (IQR)\n","def iqr_outliers(series):\n","    Q1 = series.quantile(0.25)\n","    Q3 = series.quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - 1.5 * IQR\n","    upper = Q3 + 1.5 * IQR\n","    return ((series < lower) | (series > upper)).sum() # dara true en caso de que exista outlier o false en caso de que no\n","\n","outliers_report = {col: iqr_outliers(df[col]) for col in num_features}\n","outliers_report\n","```\n","<b> Interpretación: </b>\n","* Variables con más del 30-40% de valores faltantes pueden descartarse o requerir imputación avanzada.\n","* Los outliers pueden ser naturales (casos válidos extremos) o errores; deben evaluarse con criterio de negocio.\n"," * La eliminación de duplicados mejora la calidad del entrenamiento y evita sobreponderar ciertos registros."],"metadata":{"id":"9q8UEkRCMrJ2"}},{"cell_type":"markdown","source":["### 1.4. Correlaciones y relaciones entre variables\n","\n","<b>Objetivo</b>: Identificar relaciones entre las variables numéricas y detectar redundancias o asociaciones relevantes con el target.\n","\n","El análisis de correlación permite:\n","\n","* Detectar multicolinealidad, es decir, variables que aportan la misma información.\n","* Identificar variables con fuerte relación con el target, que podrían ser buenos predictores.\n","* Exportar posibles dependencias entre vairables.\n","\n","```python\n","# Matriz de correlación\n","corr_matrix = df[num_features].corr()\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", center=0)\n","plt.title(\"Matriz de correlación - Variables numéricas\")\n","plt.show()\n","```\n","\n","<b> Interpretación:</b>\n","\n","* Correlaciones muy altas (|ρ| > 0.8) entre variables numéricas indican redundancia (multicolinealidad)\n","* En fases posteriores, una de ellas podrá eliminarse (fase de selección de variables).\n","* Correlaciones fuertes con el target orientan sobre qué variables son más influyentes en la predicción."],"metadata":{"id":"joVWE-Rm-QAy"}},{"cell_type":"markdown","source":["### 1.5. Análisis del target\n","\n","<b>Objetivo:</b> Analizar el comportamiento de la variable objetivo (target), ya que su naturaleza condiciona el tipo de modelo, las métricas de evaluación y la estrategia de entrenamiento.\n","\n","* En problemas de clasificación, interesa evaluar la proporciona de clases (balance o desbalance)\n","* En problemas de regresión, se estudio la distribución, rango, media y presencia de valroes extremos\n","\n","```python\n","# CLASIFICACIÓN : distribución de clases\n","print(\"Distribución del target (clasificación):\")\n","print(df[target_col].value_counts(normalize=True) * 100)\n","\n","df[target_col].value_counts().plot(kind=\"bar\", color=\"lightblue\", edgecolor=\"black\")\n","plt.title(\"Distribución del target\")\n","plt.xlabel(\"Clase\")\n","plt.ylabel(\"Frecuencia\")\n","plt.show()\n","```\n","\n","```python\n","# REGRESIÓN: análisis de distribución\n","print(\"Estadísticas del target (regresión):\")\n","display(df[target_col].describe())\n","\n","plt.hist(df[target_col], bins=30, color=\"skyblue\", edgecolor=\"black\")\n","plt.title(\"Distribución del target\")\n","plt.xlabel(target_col)\n","plt.ylabel(\"Frecuencia\")\n","plt.show()\n","```\n","\n","<b>Interpretación:</b>\n","\n","* Si existe desbalanceo, las métricas tradicionales (`accuracy`) no serán adecuadas, se deberán usar `recall`, `F1_score`, `Roc-AUC` y aplicar técncias de balance en la fase 7.\n","* Si el target numérica presenta asimetria o valores extremos, se valorará aplicar transfomaciones (log, Yeo-johnson, Box-Cos) en la fase de regularización."],"metadata":{"id":"bFFNNDbBPg7B"}},{"cell_type":"markdown","source":["### 1.6. Conclusiones del EDA\n","\n","El EDA finaliza con un resumen interpretativo que recoge los principales hallazgos y decisiones a tomar:\n","\n","* Variables con valores faltantes significativos = Imputación o eliminación\n","* Variables con outliers relevantes = transformaciones o revisión\n","* Variables altamente correlacionadas = posible eliminación en selección de variables.\n","* Distribución del target = revisión del balanceo o transformaciones.\n","* Variables con mayor poder predictivo = candidatas principales para el modelado"],"metadata":{"id":"03SkTO9wYOFu"}},{"cell_type":"markdown","source":["## 2. Preprocesamiento de los Datos\n","\n","El preprocesamiento es la fase donde los datos se limpian, transforman y preparan para ser utilizados por los algoritmos de Machine Learning.\n","\n","El objetivo es garantizar que los datos sean coherentes, numéricamente comparables y relevantes para el modelo.\n","\n","Un buen preprocesamiento mejora la capacidad de generalización del modelo y evita errores derivados de datos mal estructurados."],"metadata":{"id":"Ot4VcUdWYvh8"}},{"cell_type":"markdown","source":["### 2.1 Objetivo del preprocesamiento\n","\n","El propósito de esta fase es:\n","\n","* Corregir errores y valores faltantes.\n","* Estandarizar los tipos de variables\n","* Codificar adecuadamente las variables categóricas.\n","* Detectar y controlar outliers\n","* Preparar el dataset para su posterior división en entrenamiento y prueba.\n","\n","Durante esta estapa no se modifica la estructura del problema (no se crean nuevas variables no se transforman las existentes de forma compleja); esas tareas se reservan para la fase de Regularización."],"metadata":{"id":"LIkaT70nZGsx"}},{"cell_type":"markdown","source":["### 2.2. Tratamiento de valores faltantes (valores numéricos)\n","\n","Los valores nulos o faltantes (NaN) son uno de los problemas más comunes en datasets reales. Pueden deberse a errores de registro, datos perdidos o información no aplicable.\n","\n","<b> Estrategias comunas: </b>\n","* Eliminación de filas o columnas: solo si el porcentaje de nulos es muy alto (>40%) y no afectan a la representatividad.\n","* Imputación: reemplazar los valoress faltantes por una medida representativa.\n","  * Media o mediana (variables numéricas).\n","  * Moda o categoría \"Desconocido\" (varaibles categóricas).\n","\n","```python\n","# Porcentaje de valores faltantes por variable\n","missing_pct = (df.isnull().sum() / len(df)) * 100\n","missing_pct[missing_pct > 0].sort_values(ascending=False)\n","```\n","\n","```python\n","# Imputación de valores numéricos con la mediana\n","for col in num_features:\n","    df[col].fillna(df[col].median(), inplace=True)\n","\n","# Imputación de valores numéricos con la media\n","for col in num_features:\n","    df[col].fillna(df[col].mean(), inplace=True)\n","\n","# Imputación de variables categóricas con la moda\n","for col in cat_features:\n","    df[col].fillna(df[col].mode()[0], inplace=True)\n","```\n","\n","<b>Interpretación </b>\n","Las imputaciones mantienen la coherencia estadística del dataset sin eliminar información útil. En fases más vanzadas se podrá consdierar imputación mediante modelos si el patrón de nulos es complejo."],"metadata":{"id":"XJF4z8HxvpmE"}},{"cell_type":"markdown","source":["### 2.3. Detección y tratamiento de outliers. </b> (valores numéricos)\n","\n","Los outliers son valores que se alejan significativamente del resto de observaciones. Pueden representar errores de medición o casos válidos extremos.\n","\n","El tratamiento de outliers depende del contexto y del impacto que tengan en el modelo.\n","\n","* Si son errores = eliminar o reemplazar\n","* Si son valores por extremos = consdierar transformaciones logarítmicas o estandarizaciónn robusta.\n","\n","```python\n","# Revisión visual con boxplots\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","for col in num_features:\n","    plt.figure(figsize=(5, 3))\n","    sns.boxplot(x=df[col], color=\"lightblue\")\n","    plt.title(f\"Distribución y outliers en {col}\")\n","    plt.show()\n","```\n","\n","```python\n","# Detección de outliers mediante el metodo IQR\n","for col in num_features:\n","    Q1 = df[col].quantile(0.25)\n","    Q3 = df[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - 1.5 * IQR\n","    upper = Q3 + 1.5 * IQR\n","    has_outliers = ((df[col] < lower) | (df[col] > upper)).any()\n","    if has_outliers:\n","        outlier_columns.append(col)\n","\n","print(\"Columnas con outliers:\", outlier_columns)\n","```\n","\n","<b>Interpretación:</b>\n","\n","La eliminación de outliers se aplica con precaución, ya que puede reducir la representatividad. En la fase de Regularización se aplicarán transformaciones más robustas (Yeo-Johnson, Box-Cos,etx.) ara controlar su efecto sin perder información."],"metadata":{"id":"jEovfPb90qoQ"}},{"cell_type":"markdown","source":["### 2.4. Tratamiento de variables categóricas.\n","\n","Las variables categóricas representan información no numérica, por lo que deben codificarse numericamente antes del modelado.\n","\n","Dependiendo del tipo de variable y el modelo que se emplee, se utilizan diferentes estrategias de codificación.\n","\n","A continuación, se detallan las 6 técncias principales:\n","\n","\n","#### <b> Método 1: Label Encoding (codificación ordinal simple). </b>\n","\n","Convierte categorías en valores numéricos entre (0,1,2...). Es útil cuando las categorías tienen un orden lógico (por ejemplo, bajo < medio < alto).\n","No es recomendable en variables nominales, ya que introduce una relación artificial entre categorías.\n","\n","```python\n","from sklearn.preprocessing import LabelEncoder\n","\n","def label_encoder(df, columna):\n","    encoder = LabelEncoder()\n","    df[columna] = encoder.fit_transform(df[columna])\n","    return df\n","\n","```\n","* Ventajas: Simple y eficiente\n","* Desventajas: puede onducir relaciones numéricas inexsistentes\n","\n","\n","#### <b> Método 2: One-Hot Encoding (codificación ordinal simple). </b>\n","\n","Crea una nueva columna por cada categorías y asigna valores binarios (0 o 1),\n","Es el método más utilziado cuando las categorias no tienen un orden jerárquico.\n","\n","```python\n","from sklearn.preprocessing import OneHotEncoder\n","\n","def one_hot_encoding(df, columna):\n","    encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n","    encoded = encoder.fit_transform(df[[columna]])\n","\n","    encoded_df = pd.DataFrame(\n","        encoded,\n","        columns=encoder.get_feature_names_out([columna]),\n","        index=df.index\n","    )\n","\n","    df_encoded = pd.concat([df.drop(columns=[columna]), encoded_df], axis=1)\n","    \n","    return df_encoded\n","```\n","\n","* Ventajas: evita introducir jerarquíoas artificiales.\n","* Desventajas: puede aumentar mucho la dimensionalidad si hay muchas categorías.\n","\n","\n","#### <b> Método 3: Ordinal Encoding manual (codificación ordinal simple). </b>\n","\n","Se usa cuando las categorías tienen un orden natural definido por el conocimiento del negocio (por ejemplo, educación o nivel de riesgo),\n","\n","Se asignan valores manualmente a cada nivel\n","\n","```python\n","# Ejemplo: niveles de educación\n","education_map = {\n","    \"primary\": 1,\n","    \"secondary\": 2,\n","    \"tertiary\": 3\n","}\n","df[\"education_encoded\"] = df[\"education\"].map(education_map)\n","```\n","\n","* Ventajas: respeta el orden lógico.\n","* Desventajas: requiere conocimiento experto para definir los rangos.\n","\n","\n","#### <b> Método 4: Binary Encoding (codificación ordinal simple). </b>\n","\n","Convierte las categorías en números binarios y descompone esos números en columnas individuales.\n","\n","Es útil cuando hay muchas cateogrías (alta cardinalidad).\n","\n","\n","```python\n","!pip install category_encoders\n","import category_encoders as ce\n","import pandas as pd\n","\n","def binary_encode_col(df, columna):\n","    encoder = ce.BinaryEncoder(cols=[columna])\n","    df_encoded = encoder.fit_transform(df)\n","\n","    return df_encoded\n","```\n","\n","* Ventajas: Redcue la dimensionalidad comparado con One-Hot.\n","* Desventajas: menos interpretable.\n","\n","\n","#### <b> Método 5: Target(mean) Encoding (codificación ordinal simple). </b>\n","\n","Reemplaza cada categoría por la media del valor del target para esa categoría.\n","Se usa en variables categóricas con relación estadística fuerte con la variable objetivo.\n","Requiere cuidado para evitar data leakege (debe aplicarse dentro de validación cruzada.\n","\n","```python\n","def target_encode_col(df, columna, target_col):\n","    # Calcula el promedio del target por categoría\n","    target_mean = df.groupby(columna)[target_col].mean()\n","    \n","    # Mapea los valores de la columna al promedio correspondiente\n","    df_encoded = df.copy()\n","    df_encoded[columna + \"_encoded\"] = df_encoded[columna].map(target_mean)\n","    \n","    return df_encoded\n","```\n","* Ventajas: Captura relaciones predictivas directas.\n","* Desventajas: puede provocar sobreajuste si no se regula correctamente.\n","\n","#### <b> Método 6: Rare Label Encoding (codificación ordinal simple). </b>\n","\n","Agrupa las categorías poco frecuentes dentro de una misma categoría \"Otros\".\n","Se aplica antes de codificar, para evitar columnas irrelevantes o con muy poca representación.\n","\n","```python\n","def rare_encoding(df, columna, threshold=0.05, rare_label='Rare'):\n","   # Calculamos la frecuencia relativa de cada categoría\n","    freq = df[columna].value_counts(normalize=True)\n","\n","    # Seleccionamos las categorías frecuentes (las que superan el umbral)\n","    categorias_frecuentes = list(freq[freq > threshold].index)\n","\n","    # Agrupamos las categorías poco frecuentes bajo la etiqueta 'Rare'\n","    df[columna] = np.where(df[columna].isin(categorias_frecuentes), df[columna], rare_label)\n","\n","    return df\n","```\n","\n","* Ventajas: Simplifica el modelo y reduce ruido\n","* Desventajas: se pierde algo de granularidad, aunque suele mejorar la estabilidad.\n","\n","#### <b> Método 7: Frequency Encoding (Codificación por frecuencia o frecuencia relativa) </b>\n","\n","Convierte cada categoría en un valor numérico que representa su frecuencia (o proporción dentro de la columna.\n","\n","En lugar de crear nuevas columnas (como el One-Hot Encoding), este método asinga un valor numérico por categoría,\n","\n","```python\n","def frequency_encoding(df, columna):\n","  # Creamos una nueva columna llamda categoria y _ecnoded, donde mapeamos los valores con la frecuencia usando value_counts()\n","  df[columna + '_encoded'] = df[columna].map(df[columna].value_counts())\n","  return df\n","```\n","\n","* Ventajas: Reduce dimensionalidad, refleja la importancia relativa de cada categoría, eficiente en tiempo y memoria, y compativle con modelos de árbol.\n","* Desventajas: Menor interpretabilidad, posibles pérdidas en categorías raras, y riesgo de data leakeged si se calcula con datos de test.\n"],"metadata":{"id":"PpamtjGdb38h"}},{"cell_type":"markdown","source":["### 2.5 Limpieza y homogeneización final\n","\n","Una vez completadas las imputaciones y codificaciones, se reaza una revisión final para garantizar la coherencia del dataset antes del modelado.\n","\n","Pasos:\n","\n","1. Comprobar que no queden nulos ni duplicados.\n","2. Verificar que todas las columnas sean numéricas.\n","3. Homogeneizar nombres de variables (minúsculas, sin espacios)\n","4. Separar las matrices `X`(features) y `y`(target).\n","\n","```python\n","# Comprobación final de nulos\n","df.isnull().sum().sum()\n","\n","# Comprobación de duplicados\n","df.duplicated().sum()\n","\n","# Homogeneización de nombres de columnas\n","df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n","\n","# Separación de X (features) e y (target)\n","X = df.drop(columns=[target_col])\n","y = df[target_col]\n","```\n","\n","<b>Interpretación:</b>\n","\n","Este pasi garantiza que el dataset esté limpio, coherente y listo para ser dividido en conjunto de entrenamiento y prueba, que será el siguiete paso del flujo de trabajo."],"metadata":{"id":"SOm93myRghEx"}},{"cell_type":"markdown","source":["## 3. Muestreo y División del Dataset\n","\n","La fase de muestreo y división del dataset tiene como objetivo evaluar el rendimiento real del modelo, garantizando que aprenda correctamente son sobreajuste a los datos de entrenamiento.\n","\n","Dividir los datos de manera adecuada permite:\n","\n","* Evaluar la capacidad de generalziación.\n","* Evitar el *data leakage* (fugas de información).\n","* Asegurar una representación equilibrada del target.\n","\n","Esta fase es fundamental antes de entrenar cualquier modelo."],"metadata":{"id":"dSN3p4-IhQ3X"}},{"cell_type":"markdown","source":["### 3.1. Objetivo\n","\n","En todo proyecto de Machine Learning, los datos se dividen en al menos dos subconjuntos principales:\n","\n","* Train (entrenamiento): utilizado para ajustar los parámetros del modelo.\n","* Test(prueba): utilizado exclusivamente para evalaur el rendimiento final con datos nunca vistos.\n","\n","En algunos casos se añade un tercer subconjunto:\n","* Validation (validación): utilizado para seleccionar hiperparámetros o comparar modelos, sin tocar el conjunto de prueba.\n","\n","El principio fundamental es no utilizar los datos de test en ninguna etapa de entrenamiento o selección de variables, para evitar sobreajuste."],"metadata":{"id":"iz3kEy8fhuEp"}},{"cell_type":"markdown","source":["### 3.2. División básica: Train/Test Split\n","\n","El método más habitual para dividir los datos es `train_test_split` de `scikit-learn`.\n","Por defecto, se separa el 70-80% de los datos para entrenamiento y el 20-30% para prueba.\n","\n","Es recomendable establecer un `random_state` fijo para asegurar reproducibilidad entre ejecuciones.\n","\n","```python\n","from sklearn.model_selection import train_test_split\n","\n","# División en train y test (ejemplo 80/20)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,           # 20% de los datos para test\n","    random_state=42,         # semilla para reproducibilidad\n",")\n","```\n","\n","```python\n","# Comprobación de las dimensiones resultantes\n","print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape)\n","print(\"Tamaño del conjunto de prueba:\", X_test.shape)\n","```\n","\n","<b> Interpretación:</b>\n","\n","Esta división garantiza que el modelo se entrene con una muestra representativa de los datos y que se evalúe posteriormente con observaciones que no ha visto durante el aprendizaje."],"metadata":{"id":"CdeY1WqPjQUa"}},{"cell_type":"markdown","source":["### 3.3. Muestreo estratificado\n","\n","En problemas de <b>clasificación</b>, especialemnte cuando el target está desbalanceado, es importante mantener las proporciones de clases tanto en el entrenamiento como en el test.\n","\n","Para ello se utiliza el muestreo estratificado, que preserva la distribución del target en ambos conjuntos.\n","\n","```python\n","# División estratificada para mantener las proporciones del target\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    stratify=y,             # mantiene proporción de clases\n","    random_state=42         # Garantiza reproducibildiad de los datos\n",")\n","\n","# Verificación de la proporción de clases\n","print(\"Distribución de clases en TRAIN:\")\n","print(y_train.value_counts(normalize=True))\n","\n","print(\"\\nDistribución de clases en TEST:\")\n","print(y_test.value_counts(normalize=True))\n","```\n","\n","<b> Interpretación:</b>\n","\n","El muestreo estratificado evita que el modelo aprenda con una distribución distinta a la real.\n","\n","Esto es especialemnte importante cuando una clase minoritaria (por ejemplo, \"fraude\" o \"abandono de cliente\") representa un porcentaje pequeño del total."],"metadata":{"id":"VMBXx7OxkZ86"}},{"cell_type":"markdown","source":["### 3.4. Validación cruzada (Cross-Validation)\n","\n","La validación cruzada permite evaluar la estabilidad y generalziación del modelo.\n","\n","En lugar de depender de una única división train/test, el dataset se divide en K subconjuntos (folds), y el modelo se entrena y evalúa K veces, alternando los subconjuntos de valdiación.\n","\n","El rendimiento final se obtiene como la media de las métricas obtendias en cada iteración.\n","\n","<b>Tipos de validación cruzada:</b>\n","* K-fold: divide los datos en K particiones del mismo tamaño.\n","* StratifiedKFold: igual que K-Fold, pero manteniendo la proporción de clases.\n","* Leave-One-Out (LOO): usa un único registro como test en cada iteración (útil con datasets pequeños)\n","\n","```python\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Ejemplo de validación cruzada con modelo de clasificación\n","model = RandomForestClassifier(random_state=42)\n","\n","cv = StratifiedKFold(n_splits=5, # número de particiones\n","                    shuffle=True, # Indica que se deben mezclar las muestras\n","                    random_state=42) # Garantiza reproducibildiad de los datos\n","\n","scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n","\n","print(\"Resultados de validación cruzada:\", scores)\n","print(\"Media de accuracy:\", scores.mean())\n","print(\"Desviación estándar:\", scores.std())\n","```\n","\n","<b> Interpretación:</b>\n","\n","La validación cruzada es una herramienta poderosa para comparar modelos y detectar sobreajuste:\n","si el rendimiento varía mucho entre folds, el modelo podría estar aprendiendo patrones espurios o depender demasiado de una parte del dataset."],"metadata":{"id":"6HV5oA1blQWk"}},{"cell_type":"markdown","source":["### 3.5. Consideraciones adicionales\n","\n","* <b> División temporal (Times Series Split) </b>\n","\n","En problemas con datos temporales, como series de tiempo o históricos de clientes, no se debe usar muestreo aleatorio.\n","Se utiliza un Time Series Split, que respeta el orden cronológico de los datos para evitar fugas de información (data leakage temporal).\n","\n","```python\n","from sklearn.model_selection import TimeSeriesSplit\n","\n","tscv = TimeSeriesSplit(n_splits=5)\n","for train_index, test_index in tscv.split(X):\n","    print(\"Train:\", train_index, \"Test:\", test_index)\n","```\n","\n","* <b> Balanceo de conjuntos </b>\n","\n","Antes de entrenar, conviene comprobar que tanto `X_train`como `X_test` mantienen las proporciones y representatividad del dataset completo.\n","Esto garantiza que la evaluación del modelo sea justa."],"metadata":{"id":"Q6ozzJfMmQl-"}},{"cell_type":"markdown","source":["## 4. Modelado\n","\n","La fase de modelado consiste en seleccionar, entrenar y evaluar los algoritmos de Machine Learning que mejor se adapten al problema planteado.\n","\n","Su objetivo es construir un modelo capaz de aprender patrones en los datos de entrenamiento y generalizar correctamente a nuevos datos.\n","\n","Cada modelo tiene sus particularidades, ventajas y limitaciones. La elección depende de:\n","\n","* El tipo de problema (clasificación, regresión, agrupamiento (*NO SUPERVISADOS*).\n","* La cantidad y calidad de los datos.\n","* La interpretabilidad que se requiera.\n","* La complejidad de las relaciones entre variables."],"metadata":{"id":"TD2onWYYmoxU"}},{"cell_type":"markdown","source":["### 4.1. Flujo de trabajo general de modelado.\n","\n","Independientemente del algoritmo, el proceso es siempre el mismo:\n","\n","1. Definir el modelo (elegir el algoritmo y los parámetros iniciales).\n","2. Entrenar el modelo con `X_train`, `y_train`.\n","3. Predecir sobre `X_test`\n","4. Evaluar con métricas apropiadas (*se detallan en la Fase 5*).\n","\n","Ese flujo lo vamos a repetir para cada modelo."],"metadata":{"id":"OlahwRZtpEm1"}},{"cell_type":"markdown","source":["### 4.2. Modelos Lineales\n","\n","Los modelos lineales son los más sencillos y explicativos.\n","\n","Buscan encontrar una relación lineal entre las variables predictoras (features) y la variable objetivo (target). Son útiles como modelo base para establecer una referencia inicial."],"metadata":{"id":"XuD5gn5_ra3t"}},{"cell_type":"markdown","source":["#### 4.2.1. Regresión Lineal (problemas de regresión)\n","\n","La Regresión Lineal predice un valor continuo (como precio, ingresos, duración) ajustando una línea o hiperplano que minimiza el error cuadrático medio (MSE).\n","\n","$$\n","\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n\n","$$\n","\n","<b>Ventajas:</b>\n","* Rápida, interpretable y fácil de implementar.\n","* Base para otros modelos como Ridge y Lasso.\n","\n","<b>Limitaciones:</b>\n","* No capta relaciones no lineals.\n","* Sensible a outliers y multicolinealidad.\n","\n","<b>Ejemplo de uso:</b>\n","* Predecir el precio de una vivienda según sus características.\n","* Estimar las ventas mensuales en función del gasto publicitario.\n","\n","```python\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# 1. Definir modelo\n","lin_reg = LinearRegression()\n","\n","# 2. Entrenar\n","lin_reg.fit(X_train, y_train)\n","\n","# 3. Predecir\n","y_pred_lin = lin_reg.predict(X_test)\n","\n","# 4. Evaluar\n","print(\"MSE (Linear Regression):\", mean_squared_error(y_test, y_pred_lin))\n","print(\"R2 (Linear Regression):\", r2_score(y_test, y_pred_lin))\n","```\n","\n","<b>Interpretación</b>\n","\n","Si el R2 es cercano a 1, el modelo explica bien la variabilidad del target.\n","Valores bajos o errores grandes indican relaciones no lineales o presencia de ruido."],"metadata":{"id":"Ks7u0iodu8EU"}},{"cell_type":"markdown","source":["#### 4.2.2. Regresión logística (problemas de clasificación)\n","\n","La Regresión Logística estima la probabilidad de pertenencia a una clase (0 o 1) mediante una función sigmoide.\n","\n","$$\n","P(y=1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n)}}\n","$$\n","\n","<b> Ventajas: </b>\n","* Muy interpretable (coeficientes = influencia de cada variable)\n","* Útil para obtener probabilidades.\n","* Ideal como modelo base en clasificación\n","\n","<b>Limitaciones:</b>\n","* Solo modela relaciones lineales\n","* Requiere variables numéricas y escaladas\n","\n","<b> Ejemplo de uso:</b>\n","* Banca: predecir si un cliente incumplirá un crédito.\n","* Marketing: Estimar si un cliente reponderá a una campaña\n","\n","```python\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n","\n","# 1. Definir el modelo\n","log_clf = LogisticRegression(max_iter=1000, # numero maximo de iteraciones\n","                            random_state=42) # Reproducibilidad de la aleatoriedad\n","\n","# 2. Entrenar el modelo\n","log_clf.fit(X_train, y_train)\n","\n","# 3. Predecir\n","y_pred_log = log_clf.predict(X_test) # Resutlado predicho 0 o 1\n","y_proba_log = log_clf.predict_proba(X_test)[:, 1] # probabilidades de clasificación de 0 y 1\n","\n","# 4. Evaluar\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n","print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_log))\n","print(classification_report(y_test, y_pred_log))\n","```\n","\n","<b>Interpretación:</b>\n","\n","La regresión logística ofrece un equilibrio entre rendimiento e interpretabildiad, ideal como punto de partida para proyectos de clasificación."],"metadata":{"id":"g3FC8O2CrEL7"}},{"cell_type":"markdown","source":["### 4.3. Árboles deDecisión\n","\n","Los Árboles de Decisión dividen el espacio de los datos en regiones homogéneas mediante reglas *if-else*. Cada nodo representa una decisión basada en una variable, y cada hoja una predicción final.\n","\n","Son modelos no lineales que pueden manejar tanto variables numéricas como categóricas."],"metadata":{"id":"MJDINDTTsr1y"}},{"cell_type":"markdown","source":["#### 4.3.1 Árbol de Clasificación\n","\n","Los árboles de clasificación son modelos que apreden una serie de reglas de decisión a partir de los datos, organizados en una estructura jerárquica.\n","\n","En cada nodo se realiza una pregunta sobre una caracaterística y se divide el conunto de datos según la respuesta.\n","\n","El objetivo es particionar el espacio de características en regiones homogéneas respecto a la variable objetivo, de forma que en cada hoja haya observaciones predominantemente de una misma clase.\n","\n","<b>Ventajas:</b>\n","* Fácil de interpretar y visualizar\n","* Capta relaciones no lineales.\n","* No necesita escalado\n","\n","<b>Limitaciones:</b>\n","* Propenso al overfitting\n","* Sensible a pequeñas variaciones en los datos.\n","\n","<b>Ejemplo de uso:</b>\n","* Educación: predecir si un alumno aprobará o no.\n","* Salud: Clasificar tipo de tratamiento según síntomas\n","\n","```python\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Definición del modelo\n","tree_clf = DecisionTreeClassifier(\n","    criterion=\"gini\", #  Indica la métrica de impureza que el árbol usa para decidir las divisiones en los nodos\n","    max_depth=5, # Cuántas veces se puede dividir el árbol para crear ramas, Cuanto mayor sea, más complejas serán las reglas y mayor riesgo de sobreajuste\n","    min_samples_split=10, # Define el número mínimo de muestras necesarias, Aumentar este valor reduce el sobreajuste\n","    random_state=42\n",")\n","\n","# 2. Entrenamiento\n","tree_clf.fit(X_train, y_train)\n","\n","# 3. Predicción\n","y_pred_tree = tree_clf.predict(X_test)\n","\n","# 4. Evaluación\n","print(\"Accuracy (Decision Tree):\", accuracy_score(y_test, y_pred_tree))\n","```\n","\n","<b>Interpretación:</b>\n","\n","Un accuracy alto indica que el árbol clasifica correctamente la mayoría de los casos del conjunto de test. Sin embargo, si el rendimiento en entrenamiento es mucho mayor que en test, el árbol puede estar sobreajustando.\n","\n","Se recomienda analizar la importancia de las variables (`tree_clf.feature_importances_`) para identificar los atributos más relevantes y visualizar el árbol con `plot_tree()` para interpretar las reglas aprendidas."],"metadata":{"id":"Qwo9o2ZRtGL9"}},{"cell_type":"markdown","source":["#### 4.3.2. Árbol de Regresión\n","\n","Un Árbol de Regresión es un modelo que divide el espacio de las variables predictoras en regiones más homogéneas en cuanto al valor de la variable objetivo (continua).\n","\n","En cada nodo, el algoritmo selecciona la variable y el punto de corte que mejor minimiza la variabilidad del valor objetivo dentro de los grupos resultantes.\n","\n","A diferencia de un árbol de clasificación, que predice una etiqueta discreta, un árbol de regresión predice un valor numérico, normalmente el promedio de las observaciones que caen en cada hoja.\n","\n","<b>Ventajas:</b>\n","* Capta relaciones no lineales y efectos de interacción entre variables.\n","* Fácil de interpretar y visualizar mediante reglas de decisión.\n","* No requiere escalado de características ni supuestos estadísticos sobre la distribución de los datos.\n","\n","<b>Limitaciones:</b>\n","* Tiende al sobreajuste (overfitting) si no se limita la profunidad o tamaño mínimo de los nodos.\n","* Puede presentar alta varianza: pequeñas variaciones en los datos generan árboles distintos.\n","* No produce predicciones continuas suaves (las predicciones por hoja son constantes).\n","\n","<b>Ejemplo de uso:</b>\n","* Finanzas: predecir la rentabilidad esperada de una inversión.\n","* industria: estimar consumo energético según condiciones operativas,\n","\n","```python\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# 1. Definición del modelo\n","tree_reg = DecisionTreeRegressor(\n","    max_depth=5, # Cuántas veces se puede dividir el árbol para crear ramas, Cuanto mayor sea, más complejas serán las reglas y mayor riesgo de sobreajuste\n","    min_samples_split=10, # Define el número mínimo de muestras necesarias, Aumentar este valor reduce el sobreajuste\n","    random_state=42 # Semilla aleatoria reproducible\n",")\n","\n","# 2. Entrenamiento\n","tree_reg.fit(X_train, y_train)\n","\n","# 3. Predicción\n","y_pred_tree_reg = tree_reg.predict(X_test)\n","\n","# 4. Evalaución\n","print(\"MSE (Decision Tree Regressor):\", mean_squared_error(y_test, y_pred_tree_reg))\n","print(\"R2 (Decision Tree Regressor):\", r2_score(y_test, y_pred_tree_reg))\n","```\n","\n","<b>Interptación:</b>\n","\n","Un R² alto indica que el árbol explica bien la variabilidad del objetivo y un MSE bajo que los errores de predicción son pequeños. Si el rendimiento en entrenamiento es mucho mejor que en test, el árbol puede estar sobreajustando.\n","\n","Revisa la importancia de las variables (`tree_reg.feature_importances_`) para identificar los predictores más influyentes y, si lo necesitas, visualiza el árbol con `plot_tree()` para comprender las reglas aprendidas. Para reducir el sobreajuste, limita la complejidad con `max_depth`, `min_samples_split` o `min_samples_leaf`."],"metadata":{"id":"qx5XtN9ttvf1"}},{"cell_type":"markdown","source":["### 4.4. Random Forest\n","\n","El Random Forest es un ensemble de árboles de decisión.\n","\n","Cada árbol se entrena con una muestra aleatoria (botsrap) del dataset y un subconjunto de variables.\n","\n","La predicción final se obtiene por votación (clasificación) o promedio (regresión)."],"metadata":{"id":"2z7RHXRdux7G"}},{"cell_type":"markdown","source":["#### 4.4.1. Random Forest de Clasificación\n","\n","El Random Forest es un agoritmo de ensemble que combina muchos árboles de decisión para mejorar la precisión y reducir el sobreajuste de un solo árbol.\n","\n","Durante el entrenamiento, construye múltiples árboles sobre subconjuntos aleatorios de los datos y de las variables.\n","\n","Cada árbol produce una predicción, y el modelo final vota por la clase más frecuente entre todos los árboles.\n","\n","Esta combinación de múltiples modelos independientes permite obtener predicciones más estbles, precisas y robustas al ruido.\n","\n","<b>Ventajas:</b>\n","* Reduce el sobreajuste de un sólo árbol.\n","* Alta precisión y robustez\n","* Proporcion importancia de variables\n","\n","<b>Limitaciones:</b>\n","* Menor interpretabilidad\n","* Puede ser mas lento en entrenamiento y predicción si se usan muchos árboles.\n","* Tiende a favorecer variables con más categorías (en datos categóricos).\n","\n","<b>Ejemplo de uso:</b>\n","* Banca: detección de fraude.\n","* Seguros: predicción de reclamaciones fraudulentas.\n","\n","```python\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","\n","# 1. Definción del modelo\n","rf_clf = RandomForestClassifier(\n","    n_estimators=200, # Número de árboles, cuantos más árboles, más estable y precisa será la predicción\n","    max_depth=5, # profundiad máxima del árbol\n","    random_state=42, # Semilla aleatoria reproducible\n","    n_jobs=-1, # número de núcleos del procesador que se usarán, n_jobs=-1 significa que se utilizarán todos los núcleos disponibles\n","    min_samples_leaf = 5 # Número mínimo de muestras requeridas en cada hoja: valores mayores suavizan el modelo y reducen el sobreajuste\n",")\n","\n","# 2. Entrenamiento\n","rf_clf.fit(X_train, y_train)\n","\n","# 3. Predicción\n","y_pred_rf = rf_clf.predict(X_test)\n","y_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n","\n","# 4. Evaluación\n","print(\"Accuracy (Random Forest):\", accuracy_score(y_test, y_pred_rf))\n","print(\"ROC-AUC (Random Forest):\", roc_auc_score(y_test, y_proba_rf))\n","```\n","\n","<b>Intepretación:</b>\n","\n","Un accuracy alto y un ROC-AUC cercano a 1 indican que el modelo clasifica correctamente y separa bien las clases.\n","Si el rendimiento en entrenamiento es mucho mayor que en test, podría haber ligero sobreajuste, aunque en general el Random Forest lo reduce bastante.\n","Se recomienda analizar la importancia de las variables (`rf_clf.feature_importances_`) para identificar los factores más influyentes en la clasificación.\n","También puede ajustarse el número de árboles (`n_estimators`) o la profundidad máxima (`max_depth`) para equilibrar rendimiento y coste computacional."],"metadata":{"id":"l48PKf6FvAzm"}},{"cell_type":"markdown","source":["####4.4.2. Random Forest de Regresión\n","\n","El Random Forest de Regresión combina múltiples árboles de decisión entrenados sobre subconjuntos aleatorios de datos y características.\n","En lugar de votar por una clase, el modelo promedia las predicciones de los distintos árboles, reduciendo la varianza y mejorando la capacidad de generalización.\n","\n","<b>Ventajas:</b>\n","* Reduce el sobreajuste de un sólo árbol.\n","* Alta precisión y robustez\n","* Proporcion importancia de variables\n","\n","<b>Limitaciones:</b>\n","* Menor interpretabilidad\n","* Puede ser mas lento en entrenamiento y predicción si se usan muchos árboles.\n","* Tiende a favorecer variables con más categorías (en datos categóricos).\n","\n","<b>Ejemplo de uso:</b>\n","* Inmobiliario: predecir precios de vivienda.\n","* Industria: estimar la vida útil de un componente.\n","\n","```python\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# 1. Definició del modelo\n","rf_reg = RandomForestRegressor(\n","    n_estimators=200,\n","    max_depth=5,\n","    random_state=42,\n","    n_jobs=-1,\n","    min_samples_leaf = 5\n",")\n","\n","# 2. Entrenamiento\n","rf_reg.fit(X_train, y_train)\n","\n","# 3. Predicción\n","y_pred_rf_reg = rf_reg.predict(X_test)\n","\n","# 4. Evaluación\n","print(\"MSE (Random Forest Regressor):\", mean_squared_error(y_test, y_pred_rf_reg))\n","print(\"R2 (Random Forest Regressor):\", r2_score(y_test, y_pred_rf_reg))\n","```\n","\n","<b>Interpretación:</b>\n","\n","Un R² alto indica que el modelo explica bien la variabilidad del objetivo, mientras que un MSE bajo refleja que los errores de predicción son pequeños.\n","Si el rendimiento en entrenamiento es muy superior al de test, podría existir ligero sobreajuste, aunque este modelo lo mitiga bastante.\n","\n","Es recomendable revisar la importancia de las variables (`rf_reg.feature_importances_`) para identificar cuáles influyen más en la predicción y obtener información útil sobre los factores determinantes.\n","Además, se pueden ajustar parámetros como `n_estimators`, `max_depth` o `min_samples_leaf` para equilibrar el rendimiento y la complejidad del modelo."],"metadata":{"id":"6dRKK2DTv6ra"}},{"cell_type":"markdown","source":["### 4.5. Gradient Boosting\n","\n","El Gradient Boosting entrena árboles débiles de forma secuencial, donde cada nuevo árbol se ajusta a los errores (residuales) del modelo acumulado hasta el momento. En vez de promediar árboles independientes (como en bagging), los suma uno tras otro para corregir gradualmente las predicciones.\n","\n","En cada iteración se minimiza una función de pérdida (log-loss, MSE, etc.) siguiendo la dirección del gradiente, y se controla el paso con un learning rate (shrinkage). Opcionalmente, se usan subsample de filas y/o columnas para añadir aleatoriedad y reducir sobreajuste. El resultado es un modelo que capta relaciones no lineales e interacciones con gran capacidad predictiva, a costa de requerir más cuidado en el ajuste de hiperparámetros y un entrenamiento secuencial más lento."],"metadata":{"id":"kvDxYkctwfcL"}},{"cell_type":"markdown","source":["#### 4.5.1 XGBoost de Clasificación (Gradient Boosting optimizado)\n","\n","XGBoost (Extreme Gradient Boosting) es una implementación optimizada de Gradient Boosting con mejoras en eficiencia, regularización y manejo de valores faltantes. Construye árboles secuencialmente, donde cada árbol corrige los errores del anterior, pero añade regularización L1/L2 y trucos de ingeniería (paralelización, manejo interno de NaNs, shrinkage) que suelen traducirse en mejor rendimiento y menor sobreajuste.\n","\n","<b>Ventajas</b>\n","\n","* Excelente rendimiento predictivo y robustez frente a ruido.\n","\n","* Control fino del sobreajuste: `learning_rate` (eta), `max_depth`, `subsample`, `colsample_bytree`, L1/L2 (`reg_alpha`, `reg_lambda`) y early stopping.\n","* Maneja valores faltantes de forma nativa (aprende la dirección óptima del split).\n","\n","<b>Limitaciones:</b>\n","* Más sensible a ruido y mal ajuste de hiperparámetros.\n","* Entrenamiento más lento (secuencial).\n","\n","<b>Ejemplo de uso:</b>\n","* Marketing: predicción de abandono (churn).\n","* Banca: clasificación de clientes por riesgo crediticio.\n","\n","```python\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","\n","# 1. Definición del modelo\n","xgb_clf = XGBClassifier(\n","    n_estimators=300,       # nº de árboles (rondas de boosting)\n","    learning_rate=0.05,    # tasa de aprendizaje (shrinkage)\n","    max_depth=4,           # profundidad máxima de cada árbol\n","    subsample=0.8,         # fracción de filas por árbol (bagging)\n","    colsample_bytree=0.8,  # fracción de columnas por árbol (feature bagging)\n","    reg_lambda=1.0,        # regularización L2\n","    reg_alpha=0.0,         # regularización L1\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","# 2. Entrenamiento (con early stopping opcional)\n","xgb_clf.fit(\n","    X_train, y_train,\n","    eval_set=[(X_valid, y_valid)],   # si no tienes valid, usa una partición de train\n","    eval_metric=\"auc\",\n","    early_stopping_rounds=50,        # detiene si no mejora el AUC en 50 rondas\n","    verbose=False\n",")\n","\n","# 3. Predicción\n","y_pred_xgb = xgb_clf.predict(X_test)\n","y_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n","\n","# 4. Evaluación\n","print(\"Accuracy (XGBoost):\", accuracy_score(y_test, y_pred_xgb))\n","print(\"ROC-AUC (XGBoost):\", roc_auc_score(y_test, y_proba_xgb))\n","```\n","\n","<b>Interpretación:</b>\n","\n","Un accuracy alto y un ROC-AUC cercano a 1 indican buena separación de clases.\n","Si el rendimiento en *train* supera mucho al de test, ajusta la regularización (`reg_alpha`, `reg_lambda`), reduce `max_depth` o `learning_rate`, y usa early stopping.\n","Para interpretar, revisa la importancia de variables (`xgb_clf.feature_importances_`) o emplea SHAP para explicaciones más finas a nivel de instancia."],"metadata":{"id":"b-NOmpsRwssc"}},{"cell_type":"markdown","source":["####4.5.2. XGBoost de Regresión (Gradient Boosting optimizado)\n","\n","El XGBoost Regressor (Extreme Gradient Boosting) es una versión optimizada del algoritmo de Gradient Boosting orientada a problemas de regresión.\n","Funciona construyendo árboles de decisión de forma secuencial, donde cada nuevo árbol intenta corregir los errores residuales del modelo previo.\n","A diferencia del Gradient Boosting tradicional, XGBoost incorpora regularización L1 y L2, manejo automático de valores faltantes, y un entrenamiento altamente eficiente y paralelo, lo que mejora la velocidad, la estabilidad y la capacidad de generalización del modelo.\n","\n","<b>Ventajas:</b>\n","* Excelente precisión predictiva y robustez frente a ruido y outliers.\n","* Control detallado del sobreajuste mediante `learning_rate`, `max_depth`, `subsample`, `colsample_bytree` y parámetros de regularización (`reg_alpha`, `reg_lambda`).\n","* Maneja automáticamente valores faltantes sin imputación previa.\n","\n","<b>Limitaciones:</b>\n","* Requiere ajuste fino de hiperparámetros para alcanzar su máximo rendimiento.\n","* Entrenamiento secuencial (no totalmente paralelo entre árboles).\n","* Menor interpretabilidad que un árbol individual o modelos lineales.\n","\n","<b>Ejemplo de uso:</b>\n","* Economía: estimar beneficios de una empresa\n","* Energía: predecir consumo eléctrico futuro.\n","\n","```python\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# 1. Definición del modelo\n","xgb_reg = XGBRegressor(\n","    n_estimators=300,       # número de árboles (rondas de boosting)\n","    learning_rate=0.05,     # tasa de aprendizaje (shrinkage)\n","    max_depth=4,            # profundidad máxima de cada árbol\n","    subsample=0.8,          # fracción de filas por árbol\n","    colsample_bytree=0.8,   # fracción de columnas por árbol\n","    reg_lambda=1.0,         # regularización L2\n","    reg_alpha=0.0,          # regularización L1\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","# 2. Entrenamiento (opcionalmente con early stopping)\n","xgb_reg.fit(\n","    X_train, y_train,\n","    eval_set=[(X_valid, y_valid)],\n","    eval_metric=\"rmse\",\n","    early_stopping_rounds=50,\n","    verbose=False\n",")\n","\n","# 3. Predicción\n","y_pred_xgb_reg = xgb_reg.predict(X_test)\n","\n","# 4. Evaluación\n","print(\"MSE (XGBoost Regressor):\", mean_squared_error(y_test, y_pred_xgb_reg))\n","print(\"R2 (XGBoost Regressor):\", r2_score(y_test, y_pred_xgb_reg))\n","```\n","\n","<b>Intrepretación:</b>\n","\n","El XGBoost de Regresión ofrece una combinación óptima de precisión y generalización.\n","Un R² alto indica que el modelo explica correctamente la variabilidad del objetivo, mientras que un MSE bajo refleja errores de predicción reducidos.\n","Si el modelo rinde mucho mejor en entrenamiento que en test, puede existir sobreajuste, que se corrige ajustando la profundidad (`max_depth`), el `learning rate` o aplicando más regularización (`reg_alpha`, `reg_lambda`).\n","Además, la importancia de las variables (`xgb_reg.feature_importances_`) o los valores SHAP permiten interpretar qué factores influyen más en las predicciones."],"metadata":{"id":"ah0P4iQTxOKg"}},{"cell_type":"markdown","source":["### 4.6. Guardado del modelo con pickle\n","\n","Una vez elegido el mejor modelo, puede guardarse para su posterior uso sin necesidad de reentrenar haciendo uso de la libreria pickle.\n","\n","```python\n","import pickle\n","\n","# Guardar modelo entrenado\n","with open(\"modelo_final.pkl\", \"wb\") as file:\n","    pickle.dump(modelo, file)\n","\n","# Cargar modelo posteriormente\n","with open(\"modelo_final.pkl\", \"rb\") as file:\n","    modelo_cargado = pickle.load(file)\n","\n","# Verificar\n","y_pred_cargado = modelo_cargado.predict(X_test)\n","print(\"Predicciones tras cargar:\", y_pred_cargado[:5])\n","```"],"metadata":{"id":"YWjAgv9hxy5W"}},{"cell_type":"markdown","source":["## 5. Evaluación del Modelo\n","\n","Una vez entrenado el modelo, es fundamental evaluar su rendimiento para medir su capacidad de generalización y determinar si cumple los objetivos del proyecto,\n","\n","El propósito de esta fase es cuantificar la calidad de las predicciones mediante métricas específicas, según el tipo de problema.\n","\n","* Clasificación: cuando el objetivo es predecir categorías.\n","* Regresión: cuando el objetivo es predecir valores continuos.\n","\n","La elección de las métricas adecuadas es clave, ya que diferentes métricas pueden dar percepciones distintas sobre el mismo modelo."],"metadata":{"id":"EY5DTYVoyBuS"}},{"cell_type":"markdown","source":["### 5.1. Evaluación en Clasificación\n","\n","En problemas de clasificación, el modelo predice la clase (por ejemplo, \"sí\"/ \"no\") o la probabilidad de pertenecer a una clase.\n","\n","Para evaluar su rendimiento, se comparan las predicciones con los valores reales del conjunto de prueba (`y_test`)."],"metadata":{"id":"LwdkceK-yhC8"}},{"cell_type":"markdown","source":["#### 5.1.1. Matriz de confusión\n","\n","La matriz de confusión muestra cómo se distribuyen los predicciones entre las clases reales.\n","\n","|                | **Predicción positiva** | **Predicción negativa** |\n","|----------------|------------------------:|------------------------:|\n","| **Real positiva** | ✅ VP (Verdadero Positivo) | ❌ FN (Falso Negativo) |\n","| **Real negativa** | ⚠️ FP (Falso Positivo)     | ✅ VN (Verdadero Negativo) |\n","\n","<b>Interpretación:</b>\n","\n","* VP: el modelo acierta cuando dice “sí”.\n","* VN: el modelo acierta cuando dice “no”.\n","* FP: el modelo se equivoca prediciendo “sí” cuando era “no” (error tipo I).\n","* FN: el modelo se equivoca prediciendo “no” cuando era “sí” (error tipo II).\n","\n","```python\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","# Matriz de confusión\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot(cmap=\"Blues\")\n","```\n","\n","<b>Uso práctico:</b>\n","La matriz permite ver qué tipo de error es más frecuente (por ejemplo, falsos positivos en detección de fraude)."],"metadata":{"id":"1gR_Hex8zlkU"}},{"cell_type":"markdown","source":["#### 5.1.2. Métricas principales\n","\n","A partir de la matriz de confusión se derivan las sigueintes métricas:\n","\n","1. <b>Accuracy:</b> Proporciona total de aciertos. Adecuada solo si las clases están equilibradas.\n","2. <b>Precision:</b> De todos los casos que el modelo predijo como positivos, cuántos lo eran realmente. *Importante cuando el coste de un falso postivio es alto*.\n","3. <b>Recall:</b> De todos los positivos reales, cuántos detectó correctamente el modelo. Importante cuando los falsos negativos son críticos (ej. diagnóstico médico).\n","4. <b>F1-Score:</b> Media armónica entre precisión y recall. Útil cuando las clases están desbalanceadas.\n","5. <b>Curva ROC y AUC</b>\n","* ROC (Receiver Operating Characteristic): mide la capacidad del modelo para distinguir entre clases a diferentes umbrales.\n","\n","* AUC (Area Under the Curve): área bajo la curva ROC; cuanto más cercana a 1, mejor discriminación.\n","\n","```python\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    roc_curve,\n","    roc_auc_score,\n","    classification_report\n",")\n","import matplotlib.pyplot as plt\n","\n","# Cálculo de métricas\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"F1 Score:\", f1_score(y_test, y_pred))\n","print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n","\n","# Curva ROC\n","fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n","plt.figure(figsize=(6, 5))\n","plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc_score(y_test, y_proba_rf):.2f})\")\n","plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n","plt.xlabel(\"Tasa de falsos positivos (FPR)\")\n","plt.ylabel(\"Tasa de verdaderos positivos (TPR)\")\n","plt.title(\"Curva ROC\")\n","plt.legend()\n","plt.show()\n","```\n","\n","<b>Interpretación:</b>\n","* Un AUC cercano a 1.0 indica un modelo excelente.\n","* Si está cerca de 0.5, el modelo no discrimina mejor que el azar."],"metadata":{"id":"QtipJh9l0Kwm"}},{"cell_type":"markdown","source":["#### 5.1.3. Informe de clasificación\n","\n","El método `classification_report` resume todas las métricas anteriores para cada clase.\n","\n","```python\n","from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, y_pred))\n","```\n","\n","<b>Interpretación:</b>\n","\n","Permite comparar el rendimiento por clase, especialmente útil en datasets desbalanceados (por ejemplo, fraude vs no fraude)."],"metadata":{"id":"5lOj2jUG1VqV"}},{"cell_type":"markdown","source":["### 5.2. Evaluación en Regresión\n","\n","En los problemas de regresión, el modelo predice un valor continuo. Las métricas miden cuán cerca están las predicciones de los valores reales."],"metadata":{"id":"O_chESh63Mhb"}},{"cell_type":"markdown","source":["#### 5.2.1. Error Absoluto medio (MAE)\n","\n","El MAE (Mean Absolute Error) mide el error promedio absoluto entre las predicciones y los valores reales.\n","Indica cuánto se desvía, en promedio, la predicción del valor verdadero.\n","\n","\n","$$\n","MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n","$$\n","\n","```python\n","from sklearn.metrics import mean_absolute_error\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","print(f\"MAE: {mae:.3f}\")\n","```\n","\n","<b>Interpretación: </b>\n","\n","Cuanto menor sea el MAE, más precisas son las predicciones del modelo.\n","El MAE tiene las mismas unidades que la variable objetivo (por ejemplo, euros, grados, etc.)."],"metadata":{"id":"Q-klbwhF6ZOa"}},{"cell_type":"markdown","source":["#### 5.2.2. Error Cuadrático Medio (MSE)\n","\n","El MSE (Mean Squared Error) eleva al cuadrado las diferencias entre valores reales y predichos, penalizando más los errores grandes.(RMSE)\n","\n","$$\n","MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","$$\n","\n","```python\n","from sklearn.metrics import mean_squared_error\n","\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"MSE: {mse:.3f}\")\n","```\n","\n","<b>Interpretación:</b>\n","\n","El MSE da más peso a los errores grandes, por lo que es útil cuando se desea castigar más las desviaciones extremas."],"metadata":{"id":"4H7jqVyx61Tt"}},{"cell_type":"markdown","source":["#### 5.2.3. Raíz del Error Cuadrático medio (RMSE)\n","\n","El RMSE (Root Mean Squared Error) es la raíz cuadrada del MSE, y representa el error promedio en las mismas unidades que la variable objetivo.\n","\n","$$\n","RMSE = \\sqrt{MSE}\n","$$\n","\n","```python\n","import numpy as np\n","\n","rmse = np.sqrt(mse)\n","print(f\"RMSE: {rmse:.3f}\")\n","```\n","\n","<b>Interpretación:</b>\n","\n","Cuanto más bajo el RMSE, más cerca están las predicciones de los valores reales.\n","Suele ser más interpretable que el MSE, ya que usa las mismas unidades del target."],"metadata":{"id":"9q2yP0ES8KQB"}},{"cell_type":"markdown","source":["#### 5.2.4. Coeficiente de Determinación (R2)\n","\n","El R² (R-squared) mide la proporción de la variabilidad de la variable dependiente que el modelo es capaz de explicar.\n","\n","$$\n","R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n","$$\n","\n","```python\n","from sklearn.metrics import r2_score\n","\n","r2 = r2_score(y_test, y_pred)\n","print(f\"R²: {r2:.3f}\")\n","```\n","<b>Interpretación:</b>\n","\n","* R2=1: ajuste perfecto (todas las predicciones coinciden con los valores reales).\n","* R2=0: el modelo no explica nada de la variabildad.\n","* Valores negativos indican que el modelo rinde peor que una predicción promedio.\n"],"metadata":{"id":"gO4lbcVg7QR0"}},{"cell_type":"markdown","source":["## 6. Regularización y Técnicas de Mejora\n","\n","Una vez que los modelos base han sido entrenados y evaluados, el siguiente paso es mejorar su rendimiento y generalización.\n","\n","Esto implica aplicar técnicas para:\n","* Evitar el sobreajsute (overfitting)\n","* Incrementar la capacidad predictivia.\n","* Mejorar la representación de las variables.\n","\n","Esta téncicas se pueden agrupar en tres grandes bloques:\n","\n","| Bloque              | Objetivo principal                         | Técnicas                                      |\n","|---------------------|--------------------------------------------|-----------------------------------------------|\n","| 🧩 Regularización   | Controlar la complejidad del modelo        | Ridge, Lasso, ElasticNet                      |\n","| 🧠 Feature Engineering | Mejorar la información de entrada      | Creación de variables, transformación de variables |\n","| 📊 Re-muestreo      | Corregir desbalanceos o mejorar representatividad | Over-sampling, Under-sampling, muestreo estratificado |\n","\n","\n","\n"],"metadata":{"id":"1M5qECr4819U"}},{"cell_type":"markdown","source":["### 6.1. Regularización\n","\n","La regularización añade una penalización al error del modelo para evitar que los coeficientes crezcan demasiado y el modelo se sobreajuste.\n","\n","En modelos lineales, la regularización controla la complejidad limitando los pesos."],"metadata":{"id":"WovWD3Zz_HJG"}},{"cell_type":"markdown","source":["#### 6.1.1. Ridge (L2)\n","\n","Penaliza la suma de los cuadrados de los coeficientes.\n","\n","$$\n","\\text{Error} = \\text{MSE} + \\lambda \\sum_{i=1}^{n} \\beta_i^2\n","$$\n","\n","<b>Efecto: </b> fuerza a algunos coeficientes a ser exactamente 0 → útil para selección automática de variables.\n","\n","```python\n","from sklearn.linear_model import Lasso\n","\n","lasso = Lasso(alpha=0.01)\n","lasso.fit(X_train, y_train)\n","y_pred_lasso = lasso.predict(X_test)\n","\n","print(\"MSE (Lasso):\", mean_squared_error(y_test, y_pred_lasso))\n","print(\"R² (Lasso):\", r2_score(y_test, y_pred_lasso))\n","```"],"metadata":{"id":"kWOVNbE4_U7Y"}},{"cell_type":"markdown","source":["#### 6.1.2. Lasso (L1)\n","\n","Penaliza la suma absoluta de los coeficientes.\n","\n","$$\n","\\text{Error} = \\text{MSE} + \\lambda \\sum_{i=1}^{n} |\\beta_i|\n","$$\n","\n","<b>Efecto:</b> fuerza a algunos coeficientes a ser exactamente 0 → útil para selección automática de variables.\n","\n","```python\n","from sklearn.linear_model import Lasso\n","\n","lasso = Lasso(alpha=0.01)\n","lasso.fit(X_train, y_train)\n","y_pred_lasso = lasso.predict(X_test)\n","\n","print(\"MSE (Lasso):\", mean_squared_error(y_test, y_pred_lasso))\n","print(\"R² (Lasso):\", r2_score(y_test, y_pred_lasso))\n","```"],"metadata":{"id":"EbKjmIvk__i0"}},{"cell_type":"markdown","source":["#### 6.1.3. ElasticNet (L1 + L2)\n","\n","Combina ambas penalizaciones (L1 + L2).\n","\n","$$\n","\\text{Error} = \\text{MSE} + \\lambda_1 \\sum_{i=1}^{n} |\\beta_i| + \\lambda_2 \\sum_{i=1}^{n} \\beta_i^2\n","$$\n","\n","<b>Ventaja:</b> equilibrio entre regularización y selección de variables.\n","\n","```python\n","from sklearn.linear_model import ElasticNet\n","\n","elastic = ElasticNet(alpha=0.01, l1_ratio=0.5)\n","elastic.fit(X_train, y_train)\n","y_pred_elastic = elastic.predict(X_test)\n","\n","print(\"MSE (ElasticNet):\", mean_squared_error(y_test, y_pred_elastic))\n","print(\"R² (ElasticNet):\", r2_score(y_test, y_pred_elastic))\n","```\n","\n","<b>Interpretación general:</b>\n","* Si hay colinealidad, usa Ridge.\n","* Si hay muchas variables irrelevantes, usa Lasso.\n","* Si necesitas un balance, usa ElasticNet."],"metadata":{"id":"akujeqK3ATfw"}},{"cell_type":"markdown","source":["### 6.2. Feature Engineering\n","\n","El feature engineering consiste en crear, transformar o seleccionar variables para mejorar la capacidad predictiva del modelo.\n","\n","Una buena ingeniería de características puede mejorar el rendimiento tanto como cambiar de modelo."],"metadata":{"id":"ChDSIENiAwhz"}},{"cell_type":"markdown","source":["#### 6.2.1 Creación de variables (Numéricas)\n","\n","Crear nuevas variables a partir de reglas, interacciones o combinaciones matemáticas.\n","\n","<b>Método 1: Binning Encoding (Creación mediante reglas y variables binarias) </b>\n","\n","```python\n","# Ejemplo: crear variable binaria según condición\n","df[\"es_joven\"] = np.where(df[\"edad\"] < 30, 1, 0)\n","df[\"es_mayor\"] = np.where(df[\"edad\"] > 60, 1, 0)\n","````\n","\n","<b> Método 2: Agrupaciones matemáticas o ratios </b>\n","\n","```python\n","# Crear ratios e interacciones\n","df[\"ingresos_por_edad\"] = df[\"ingresos\"] / df[\"edad\"]\n","df[\"gasto_total\"] = df[\"gasto_alimentos\"] + df[\"gasto_vivienda\"] + df[\"gasto_ocio\"]\n","````\n","\n","<b>Ejemplo de uso:</b>\n","* Finanzas: crear variables de ratio deuda/ingresos.\n","* Marketing: gasto medio por cliente.ç\n","* Sanidad: índice de masa corporal (IMC = peso / altura²)."],"metadata":{"id":"lciBHgAmA5sB"}},{"cell_type":"markdown","source":["#### 6.2.2 Transformaciones de variables (Categóricas)\n","\n","Se aplican para mejorar la distribución de los datos (reducir asimetría o normalizar).\n","\n","<b> Método 1: Yeo-Johnson</b>\n","\n","Transformación aplicable a variables positivas y negativas.\n","\n","```python\n","from sklearn.preprocessing import PowerTransformer\n","\n","pt = PowerTransformer(method='yeo-johnson')\n","X_train_trans = pt.fit_transform(X_train)\n","```\n","\n","<b> Método 2: Box-Cox (solo variables positivas)</b>\n","\n","```python\n","pt_boxcox = PowerTransformer(method='box-cox')\n","X_train_boxcox = pt_boxcox.fit_transform(X_train)\n","```\n","\n","<b> Método 3: Estandarización (Z-score) </b>\n","\n","```python\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","```\n","\n","<b> Método 4: Normalización (Min-Max Scaling) </b>\n","\n","```python\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","X_train_norm = scaler.fit_transform(X_train)\n","```\n","\n","<b> Método 5: Discretización (KBinsDiscretizer)</b>\n","\n","```python\n","from sklearn.preprocessing import KBinsDiscretizer\n","\n","kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n","X_binned = kbd.fit_transform(X_train)\n","```\n","\n","<b>Interpretación:</b>\n","\n","Estas transformaciones pueden ayudar a que los modelos lineales y basados en distancias (como KNN o SVM) funcionen mejor al homogenizar escalas o distribuciones.\n","\n"],"metadata":{"id":"wBzbDjLjBRsx"}},{"cell_type":"markdown","source":["### 6.3. Control de Mutlicolinealidad\n","\n","La multicolineadliad aparece cuando varias variables numéricas están correlacionadas entre sí. Esto implica modelos lineales y puede inflar coeficientes.\n","\n","Pasos típicos:\n","\n","1. Revisar la matriz de correlación.\n","2. Eliminar una de las variable sen pares con |ρ| muy alto (> 0.8 / 0.9).\n","\n","```python\n","corr = X.corr().abs()\n","upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n","to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n","\n","print(\"Variables a eliminar por alta correlación:\", to_drop)\n","X_reduced = X.drop(columns=to_drop)\n","```\n","\n","<b>Efecto:</b> modelo más estable, menos redundante."],"metadata":{"id":"tGNHbzyuQjNH"}},{"cell_type":"markdown","source":["## 7. Optimización del Modelo y Selección de Variables\n","\n","Una vez que el modelo base ha sido entrenado y evaluado, la siguiente fase consiste en optimizar su rendimiento, reducir su complejidad innecesarua y maximizar su capacidad de generalización."],"metadata":{"id":"zgKuwg9qRFft"}},{"cell_type":"markdown","source":["### 7.1. Feature Selection (Selección de Variables)\n","\n","El objetivo de la selección de variables es reducir la dimensionalidad del conjunto de datos sin perder información útil.\n","\n","Esto mejora la interpretabilidad, reduce el riesgo de sobreajuste y acelera el enrenamiento.\n","\n","#### <b> Método 1: Importancia de variables (Feature Importances)  </b>\n","\n","Los *modelos basados en árboles* (Decisión Tree, Random Forest, Gradient Boosting) calculan automáticamente la importancia de cada variable durante el entrenamiento.\n","\n","```python\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","importances = pd.Series(modelo.feature_importances_, index=X.columns)\n","importances.sort_values(ascending=True).plot(kind='barh', figsize=(8,6), color='teal')\n","plt.title('Importancia de las variables - Random Forest')\n","plt.show()\n","```\n","<b>Interpretación:</b>\n","\n","Las variables con mayor peso son las que más influyen en la predicción.\n","Podemos eliminar las que tienen una importancia cercana a 0, ya que no aportan información útil.\n","\n","#### <b> Método 2: SelectKBest  </b>\n","\n","Selecciona autmáticamente las K variables más relevantes según una prubea estadística (ANOVA F, Chi^2, etc.)\n","\n","Muy útil para *modelos lineales o logístico*.\n","\n","```python\n","from sklearn.feature_selection import SelectKBest, f_classif\n","\n","selector = SelectKBest(score_func=f_classif, k=10)\n","X_new = selector.fit_transform(X, y)\n","\n","selected_features = X.columns[selector.get_support()]\n","print(\"Variables seleccionadas:\", list(selected_features))\n","```\n","<b>Interpretación:</b>\n","\n","Este método selecciona las variables con mayor capacidad de discriminación según la ,étrica estadística elegida.\n","\n","#### <b> Método 3: Variance Threshold  </b>\n","\n","Elimina variables con varianza casi nula (constantes o casi constantes), ya que no aportan información al modeo.\n","\n","```python\n","from sklearn.feature_selection import VarianceThreshold\n","\n","vt = VarianceThreshold(threshold=0.01)\n","X_reduced = vt.fit_transform(X)\n","print(\"Número de variables retenidas:\", X_reduced.shape[1])\n","```\n","\n","<b>Interpretación:</b>\n","\n","Variables con muy poca variabildiad no ayudan a separar clases ni a explicar el target, por lo que se eliminan para simlificar el modelo."],"metadata":{"id":"XpsI2LwfUc-e"}},{"cell_type":"markdown","source":["### 7.2. Ajuste de Hiperparámetros (Hyperparameter Tuning)\n","\n","Los hiperparámetros son configuraciones externas del modelo (como la profunidad de los árboles o el número de estimadores) que afectan su capacidad de aprendizaje.\n","\n","El objetivo es encontrar la combinación óptima que maximice el rendimiento del modelo sin sobreajustar.\n","\n","#### <b> Método 1: GridSearchCV </b>\n","\n"," Busca de manera exhaustiva entre todas las combinaciones posibles de hiperparámetros definidos.\n","\n","```python\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier # Como ejemplo de codigo\n","\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [3, 5, 10, None],\n","    'min_samples_split': [2, 5, 10]\n","}\n","\n","grid_search = GridSearchCV(\n","    estimator=RandomForestClassifier(random_state=42),\n","    param_grid=param_grid,\n","    cv=5,\n","    scoring='roc_auc',\n","    n_jobs=-1\n",")\n","\n","grid_search.fit(X_train, y_train)\n","print(\"Mejores parámetros:\", grid_search.best_params_)\n","print(\"Mejor ROC-AUC:\", grid_search.best_score_)\n","```\n","\n","<b>Interpretación:</b>\n","* cv=5 aplica validación cruzada (divide el train en 5 partes).\n","* scoring='roc_auc' usa la métrica AUC para optimizar el rendimiento.\n","* El modelo con la mejor combinación se puede guardar como modelo final.\n","\n","#### <b> Método 2: RandomizedSearchCV </b>\n","\n","En lugar de probar todas las combinaciones, prueba un número limitado de configuraciones aleatorias, reduciendo el tiempo de cómputo.\n","\n","```python\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import randint\n","\n","param_dist = {\n","    'n_estimators': randint(100, 500),\n","    'max_depth': [3, 5, 10, None],\n","    'min_samples_split': randint(2, 10)\n","}\n","\n","random_search = RandomizedSearchCV(\n","    estimator=GradientBoostingClassifier(random_state=42),\n","    param_distributions=param_dist,\n","    n_iter=20,\n","    cv=5,\n","    scoring='roc_auc',\n","    n_jobs=-1\n",")\n","\n","random_search.fit(X_train, y_train)\n","print(\"Mejores parámetros:\", random_search.best_params_)\n","print(\"Mejor ROC-AUC:\", random_search.best_score_)\n","```\n","\n","<b>Interpretación:</b>\n","* Más rápido que GridSearchCV.\n","* Útil cuando el espacio de parámetros es grande.\n"],"metadata":{"id":"tz49kiDvXLud"}},{"cell_type":"markdown","source":["### 7.3. Balanceo de Datos\n","\n","En muchos problemas de clasificación, el target puede estar desbalanceado, es decir, que una de las clases tenga muchas más observaciones que la otra (por ejemplo, “no contrata producto” ≫ “sí contrata producto”).\n","Este desbalance puede provocar que el modelo aprenda a favorecer la clase mayoritaria, reduciendo su capacidad para detectar correctamente los casos minoritarios.\n","\n","El objetivo del balanceo es garantizar que el modelo aprenda de ambas clases de forma equilibrada, manteniendo la representatividad del fenómeno real.\n","\n","#### <b> Método 1: Under-sampling </b>\n","\n","Selecciona aleatoriamente una muestra de la clase mayoritaria.\n","\n","```python\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","rus = RandomUnderSampler(random_state=42)\n","X_res, y_res = rus.fit_resample(X, y)\n","\n","print(\"Distribución tras Under-sampling:\")\n","print(y_res.value_counts(normalize=True))\n","```\n","\n","<b>Ventajas:</b> Simple y rápido\n","\n","<b>Desventajas:</b> Puede eiminar información útil de la clase mayoritaria.\n","\n","#### <b> Método 2: Over-sampling </b>\n","\n","Duplica o sintetiza observaciones de la clase minoritaria (solo si el desbalance es severo).\n","\n","```python\n","from imblearn.over_sampling import SMOTE\n","\n","smote = SMOTE(random_state=42)\n","X_res, y_res = smote.fit_resample(X, y)\n","\n","print(\"Distribución tras Over-sampling:\")\n","print(y_res.value_counts(normalize=True))\n","```\n","\n","<b>Ventajas:</b> mantiene toda la infromación original.\n","\n","<b>Desventajas:</b> puede intrducir ruido si se abusa.\n"],"metadata":{"id":"Jqv9zswlYtbJ"}}]}