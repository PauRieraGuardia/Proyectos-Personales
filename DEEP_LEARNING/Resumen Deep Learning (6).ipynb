{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["qZgrZyL3c2iv","oYmWJkwCRJAH","ibWa77yQ5f8Y","LK0CJ0B2-F63","EDyVmaS_Z200","tKUgO-y11mJ6","Ma0wP_6D5p5K","yN8AYD6i4tJt","EcRgu0urFtQc","zOYuKppLGnKX"],"authorship_tag":"ABX9TyN5rpvmrTJESU+LJ5+waJJs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <b> 1. Dense Neural Network (DNN) model in Keras </b>\n","\n","Una Red Neuronal Densa (ANN – Artificial Neural Network) es el tipo más simple de red neuronal artificial.\n","Está compuesta por capas totalmente conectadas, donde cada neurona de una capa se conecta con todas las neuronas de la siguiente.\n","Se utiliza en problemas de clasificación, regresión o predicción, donde los datos de entrada pueden representarse como <b>vectores de características. (datos estructurados)</b>\n","\n","#### <b>Estructura básica de una red neuronal densa</b>\n","\n","Una arquitectura de red neuronal densa consta de cuatro partes principales:\n","\n","* Capa de entrada (Input layer): recibe los datos iniciales.\n","\n","* Capas ocultas (Hidden layers): realizan transformaciones no lineales.\n","\n","* Capa de salida (Output layer): genera la predicción final."],"metadata":{"id":"gdxxNwtyQb5K"}},{"cell_type":"markdown","source":["## Librerias necesarias\n","\n","Antes de comenzar a construir una red neuronal, debemos importar las librerías esenciales para definir, entrenar y analizar el modelo.\n","Estas librerías cubren distintos aspectos: modelado, preprocesamiento, visualización y optimización."],"metadata":{"id":"1iiIeJvBR06l"}},{"cell_type":"code","source":["import tensorflow as tf                                     # Framework principal\n","from tensorflow.keras import layers, models, optimizers     # Definición y compilación de modelos\n","from tensorflow.keras import callbacks                      # Herramientas de control durante el entrenamiento\n","import numpy as np                                          # Operaciones numéricas y arrays\n","import pandas as pd                                         # Manejo de datasets estructurados (CSV, DataFrames)\n","import matplotlib.pyplot as plt                             # Visualización de métricas y resultados\n","from sklearn.model_selection import train_test_split        # División del dataset en train/test\n","from sklearn.preprocessing import StandardScaler            # Normalización y escalado de variables numéricas\n","import keras_tuner as kt                                    # Búsqueda automática de hiperparámetros"],"metadata":{"id":"dKfVE9F3R6hm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.1 Fase de Construcción\n","La construcción del modelo consiste en definir la arquitectura de la red neuronal, es decir, su estructura interna:\n","qué capas tendrá, cuántas neuronas incluirá y qué funciones de activación aplicará."],"metadata":{"id":"yY-JHe-idIsA"}},{"cell_type":"markdown","source":["### 1.1.1. Capa de entrada\n","\n","La capa de entrada define la forma de los datos que el modelo recibirá. No realiza cálculos ni activaciones, solo distribuye la información hacia las capas ocultas.\n","\n","con la API Funcional\n","\n","```python\n","inputs = layers.Input(shape=(n_features,), name='input_layer')\n","```\n","\n","Con la API Secuencial:\n","\n","```python\n","model = models.Sequential()\n","model.add(layers.Input(shape=(n_features,), name='input_layer'))\n","```"],"metadata":{"id":"ZdPuptmORQbQ"}},{"cell_type":"markdown","source":["### 1.1.2. Capas ocultas (Hidden Layers)\n","\n","Las capas ocultas son el núcleo de la red neuronal. Cada neurona combina las entradas mediante una suma ponderada más un sesgo (bias), y aplica una <b>función de activación</b> que introduce no linealidad, permitiendo a la red aprender relaciones complejas.\n","\n","con la API Funcional:\n","\n","```python\n","hidden_1 = layers.Dense(units=128, activation='relu', name='hidden_1')(inputs)\n","hidden_2 = layers.Dense(units=64, activation='relu', name='hidden_2')(hidden_1)\n","```\n","\n","Con la API Secuencial\n","```python\n","model.add(layers.Dense(128, activation='relu', name='hidden_1'))\n","model.add(layers.Dense(64, activation='relu', name='hidden_2'))\n","```\n"],"metadata":{"id":"Qpj8SEx8THtX"}},{"cell_type":"markdown","source":["#### Funciones de activación en capas ocultas\n","\n","Las funciones de activación son parte estructural de las capas ocultas. Sin ellas, la red sería equivalente a una regresión lineal.\n","\n","Encontramos las siguientes funciones de activación:\n","\n","``ReLU (Rectified Linear Unit)``\n","- Fórmula: `f(x) = max(0, x)`  \n","- Evita el problema del gradiente desvanecido y acelera el entrenamiento.  \n","- Uso típico: capas ocultas (la más utilizada).\n","- Ejemplo de caso de uso: En un modelo de regresión para predecir precios de viviendas,.\n","\n","Con la API Funcional\n","```python\n","x = layers.Dense(128, activation='relu')(inputs)\n","x = layers.Dense(64, activation='relu')(x)\n","```\n","\n","Con la API Secuencial\n","```python\n","layers.Dense(128, activation='relu', name='hidden_1'),\n","layers.Dense(64, activation='relu', name='hidden_2'),\n","```\n"],"metadata":{"id":"GBGGIQq3UhQv"}},{"cell_type":"markdown","source":["### 1.1.3. Capa de salida (Output Layer)\n","\n","La capa de salida produce la predicción final del modelo. Su número de neuronas y función de activación depende del tipo de problema que se quiera resolver."],"metadata":{"id":"s-yNRHkgZ_pP"}},{"cell_type":"markdown","source":["#### Funciones de activación en capas de salida (Output Layers)"],"metadata":{"id":"80tdJ5-daNtG"}},{"cell_type":"markdown","source":["##### <b> Clasificación binaria </b>\n","`Sigmoid`\n","\n","* Devuelve valores entre 0 y 1.\n","\n","* Se utiliza para estimar la probabilidad de pertenecer a una clase.\n","\n","* Se combina con la función de pérdida binary_crossentropy.\n","\n","* Ejemplo: Clasificar correos como spam (1) o no spam (0).\n","\n","con la API Funcional\n","```python\n","output = layers.Dense(1, activation='sigmoid', name='output')(x)\n","````\n","\n","con la API Secuencial\n","```python\n","layers.Dense(1, activation='sigmoid', name='output')\n","````\n","\n","##### <b> Clasificaión multiclase </b>\n","\n","``Softmax ``\n","\n","* Convierte las salidas en probabilidades que suman 1.\n","\n","* Cada neurona representa una clase distinta.\n","\n","* Se combina con la función de pérdida categorical_crossentropy.\n","\n","* Ejemplo: Clasificación de flores en tres categorías (setosa, versicolor, virginica).\n","\n","con la API Funcional\n","```python\n","output = layers.Dense(numero de clases, activation='softmax', name='output')(x)\n","````\n","\n","con la API Secuencial\n","```python\n","layers.Dense(numero de clases, activation='softmax', name='output')\n","````\n","\n","##### <b> Regresión</b>\n","\n","`Tanh` (Tangente hiperbólica) o Linear\n","\n","* Devuelve valores entre -1 y 1, útil si los valores objetivo están normalizados.\n","\n","* Se combina con funciones de pérdida como mse o mae.\n","\n","* Ejemplo: Predicción de un valor continuo previamente normalizado a [-1, 1].\n","\n","con la API Funcional\n","```python\n","output = layers.Dense(1, activation='tanh', name='output')(x)\n","\n","````\n","\n","con la API Secuencial\n","```python\n","layers.Dense(1, activation='tanh', name='output')\n","````\n","`linear`(sin activación)\n","* No aplica ninguna función de activación, por lo que la salida puede tomar cualquier valor real (positivo o negativo).\n","* Es la opción más común en problemas de regresión, donde el modelo debe predecir valores continuos sin límite.\n","* Se combina habitualmente con funciones de pérdida como MSE (mean squared error) o MAE (mean absolute error).\n","* Ejemplo: Predicción del precio de una vivienda o temperatura.\n","\n","con la API Funcional\n","```python\n","output = layers.Dense(1, activation='linear', name='output')(x)\n","````\n","\n","con la API Secuencial\n","```python\n","layers.Dense(1, activation='linear', name='output')\n","````\n"],"metadata":{"id":"GrlJzz5jaUw3"}},{"cell_type":"markdown","source":["### 1.1.4. Construcción completa del modelo."],"metadata":{"id":"y5jbu0ktcmDY"}},{"cell_type":"code","source":["# API funcional\n","inputs = layers.Input(shape=(10,), name='input')\n","x = layers.Dense(128, activation='relu', name='hidden_1')(inputs)\n","x = layers.Dense(64, activation='relu', name='hidden_2')(x)\n","output = layers.Dense(3, activation='softmax', name='output')(x)\n","\n","model = models.Model(inputs=inputs, outputs=output, name='ANN_model')\n","model.summary()"],"metadata":{"id":"IR58BU9Gcg3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# API Secuencial:\n","model = models.Sequential([\n","    layers.Input(shape=(10,)),\n","    layers.Dense(128, activation='relu', name='hidden_1'),\n","    layers.Dense(64, activation='relu', name='hidden_2'),\n","    layers.Dense(3, activation='softmax', name='output')\n","])\n","model.summary()"],"metadata":{"id":"-CkdQnKWcup6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2 Fase de compilación\n","\n","Una vez construida la arquitectura de la red neuronal (definidas las capas, sus funciones de activación y conexiones), el siguiente paso es compilar el modelo.\n","\n","Durante la compilación, se especifica cómo el modelo va a aprender. Para ello, se definen tres elementos esenciales:\n","\n","1. La función de péridda (loss): indica qué mide el modelo y qué debe minimizar\n","2. El opimizador (optimizer): indica cómo se actualizan los pesos para reducir la pérdida.\n","3. Las métricas (metrics): sirven para moniotirizar el rendimiento durante el entrenamiento, pero no afectan al aprendizaje."],"metadata":{"id":"UQaiDkvgeSgo"}},{"cell_type":"markdown","source":["### <b>Función de pérdida (loss)</b>\n","\n","La función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. El objetivo del entrenamiento es minizimar esta función.\n","\n","Dependiendo del tipo de problema, se usa una función diferente:\n","\n","<b> Classificación binaria </b>\n","\n","* Activación de salida: `sigmoid`\n","* Función de pérdida: `binary_crossentropy`\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","```\n","\n","<b> Clasificación multiclase </b>\n","\n","* Activación de salida: `softmax`\n","* Pérdida:`categorical_crossentropy` (si las etiquetas están codificadas en one-hot) o `sparse_categorical_crossentropy` (si las etiquetas son enteras)\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","```\n","\n","<b> Regresión\n","* Activación de salida: `linear`\n","* Pérdida: `mse`(eror cuadrático medio) o `mae`(arror absoluto medio)\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='mse',       # o 'mae'\n","    metrics=['mae']\n",")\n","\n","```\n"],"metadata":{"id":"1SLAugiQEY2p"}},{"cell_type":"markdown","source":["### <b> Optimizador </b>\n","\n","El optimizador define cómo se ajustan los pesos del modelo en cada iteración.\n","Su función es minimizar la pérdida mediante el descenso del gradiente u otros métodos adaptativos.\n","\n","Los más comunes son:\n","\n","* `sgd`: Gradiente descendente estocástico\n","```python\n","model.compile(optimizer='sgd', loss='mse', metrics=['mae'])\n","```\n","* `adam`: Método adaptativo, rápido y estable (recomendado por defecto)\n","```python\n","model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","```\n","* `rmsprop`: Ideal para redes recurrentes o datos no estacionarios\n","```python\n","model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","```"],"metadata":{"id":"--D4UQucPacf"}},{"cell_type":"markdown","source":["### <b> Métricas (metrics) </b>\n","\n","Las métricas permiten evaluar el rendimiento del modelo durante el entrenamiento y la validación. No influeyn en el proceso de optimización, solo sirven como referencia.\n","\n","Ejemplos:\n","\n","* `accuracy`: Clasificación (binaria o multiclase): Mide el porcentaje de aciertos\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","```\n","* `mae`: Regresión: Mide el promedio del valor absoluto de los errores\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='mse',          # pérdida puede ser MSE\n","    metrics=['mae']      # seguimiento del error medio absoluto\n",")\n","```\n","* `mse`: Regresión: Calcula el promedio de los errores al cuadrado, por lo que penaliza más los errores grandes.\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='mse',        # pérdida igual que la métrica\n","    metrics=['mse']\n",")\n","```"],"metadata":{"id":"QbaqDcIlQnkD"}},{"cell_type":"markdown","source":["## 1.3 Fase de entrenamiento\n","\n","Una vez que el modelo ha sido construido (definición de capas y activación) y compilado (selección de la función de pérdida, el optimizador y las métricas), llega el momento clave: la fase de entrenamiento.\n","\n","Durante esta estapa, el modelo aprende de los datos, ajustando sus pesos internos para minimizar la función de pérdida definida en la fase de compilación."],"metadata":{"id":"Xpv2JqPWSGXQ"}},{"cell_type":"markdown","source":["### <b> Objetivo del entrenamiento </b>\n","\n","El objetivo principal del entrenamiento es minimizar la pérdida (`loss`). Para ello, el modelo repite un cilo de pasos en cada época (`epoch`):\n","\n","1. Realiza predicciones sobre los datos de entrenamienot.\n","2. Calcula el error (pérdida) entre las predicciones y los valores reales,.\n","3. Ajusta los pesos mediante el optimizador para reducir ese error.\n","4. Evalúa el rendimiento con los datos de validación (si se han definido)"],"metadata":{"id":"4FWgp8x8TkLg"}},{"cell_type":"markdown","source":["### <b> Método </b>`model.fit()`\n","\n","En keras, el entrenamiento se realiza con el método `fit()`, que ajusta el modelo a los datos de entrada.\n","\n","```python\n","history = model.fit(\n","    x_train, y_train,      # Datos de entrenamiento\n","    epochs=20,             # Número de iteraciones completas sobre los datos\n","    batch_size=32,         # Tamaño de cada lote de entrenamiento\n","    validation_split=0.2,  # Porcentaje de datos usados para validación\n","    verbose=1              # Nivel de detalle del entrenamiento\n",")\n","```\n","\n","### <b> Parámetros principales </b>\n","\n","* `epochs`: número de veces que el modelo procesa todo el conjunto de entrenamiento. *Auméntalo si el modelo aún no ha aprendido lo suficiente (la pérdida sigue bajando);\n","redúcelo si el modelo empieza a sobreajustar (la pérdida de validación empeora).*\n","* `batch_size`: número de muestras procesadas antes de actualizar los pesos del modelo. *Auméntalo si el entrenamiento es inestable o tienes muchos datos y buena memoria;\n","redúcelo si el modelo no generaliza bien o el hardware es limitado.*\n","* `validation_split`: fracción del conjunto de entrenamiento reservada para validación (opcional). *Auméntalo si dispones de muchos datos y quieres una validación más fiable;\n","redúcelo si el conjunto de entrenamiento es pequeño y necesitas más muestras para aprender.*\n","* `validation_data`: alternativa a `validation_split`, permite especificar manualmente un conjunto `(x_val, y_val)`. *Auméntalo si dispones de muchos datos y quieres una validación más fiable;\n","redúcelo si el conjunto de entrenamiento es pequeño y necesitas más muestras para aprender.*\n","* `verbose`: controla la salida del proceso (0=silencio, 1=barra de progreso, 2=resumen por época). *Ponlo en 1 (barra de progreso) para entrenamientos normales y seguimiento visual;\n","úsalo en 2 si solo quieres un resumen por época (entrenamientos largos o en notebooks);\n","ponlo en 0 cuando no necesites mostrar información (por ejemplo, en ejecuciones automatizadas).*\n","\n","### <b> Objeto </b>\n","\n","El método `fit()` devuelve un objeto llamado `history`, que guarda la evolución de las métricas durante el entrenamiento y validación.\n","\n","Este objeto se utiliza posteriormente para analizar el rendimiento del modelo.\n","\n","```python\n","print(history.history.keys())\n","# Ejemplo de salida: dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n","```"],"metadata":{"id":"H9ffFIlIJ9xU"}},{"cell_type":"markdown","source":["## 1.4 Fase de evaluación\n","\n","Una vez que el modelo ha sido entrenado, el siguiente paso consiste en evaluar su rendimiento. El objetivo de esta fase es comprobar cómo se comporta el modelo con daatos que no ha visto durante el entrenamiento, es decir, su capacidad de generalización."],"metadata":{"id":"C-Sm7KZKUzfo"}},{"cell_type":"markdown","source":["### <b> Objeto de la evaluación </b>\n","\n","Durante el entrenamiento, el modelo mejora ajustando sus pesos para minimizar la función de péridda. Sin embargo, un modelo puede aprender demasiado bien los datos de entrenamiento ( fenómeno conocido como <b>overfitting</b>), perdiendo capacidad para generalizar.\n","\n","Por eso, en esta fase se evalúa el modelo sobre un conjunto de datos de prueba (test) o de validación no usados durante el entrenamiento,"],"metadata":{"id":"5DQc923IMwR8"}},{"cell_type":"markdown","source":["#### <b> Método `model.evaluate()`\n","\n","Keras proporciona el método `evaluate()`  para medir la pérdida y las métricas definidas en la compilación.\n","\n","```python\n","results = model.evaluate(X_test,\n","                        y_test, # Datos de prueba\n","                        verbose=1) # Nivel de detalle de la salida\n","print('Test Loss:', results)\n","```"],"metadata":{"id":"moPAdi67NOIF"}},{"cell_type":"markdown","source":["### <b> Visualización del entrenamiento </b>\n","\n","El objeto `history` devuelto por el `model.fit()` contiene el registro del proceso de aprendizaje. podemos usarlo para visualizar cómo evolucionaron la pérdida y la precisión a lo largo de las épocas. Aplicando la siguiente formula veremos el gráfico.\n","\n","```python\n","def show_loss_evolution(history):\n","  hist = pd.DataFrame(history.history)\n","  hist['epoch'] = history.epoch\n","\n","  plt.figure(figsize=(12,6))\n","\n","  plt.xlabel('Epoch')\n","  plt.ylabel('MSE')\n","  plt.plot(hist['epoch'],hist['loss'],label='Train Error')\n","  plt.plot(hist['epoch'],hist['val_loss'],label='Val Error')\n","  plt.grid()\n","  plt.legend()\n","  plt.show()\n","```\n","\n","Interpretación:\n","\n","| **Patrón observado** | **Interpretación** | **Posible acción** |\n","|------------------------|--------------------|--------------------|\n","| Ambas bajan y se estabilizan | Aprendizaje correcto | Mantener configuración |\n","| `loss ↓` pero `val_loss ↑` | Overfitting | Aplicar `Dropout`, `EarlyStopping` o regularización (`L1`/`L2`) |\n","| Ambas altas y sin mejora | Underfitting | Aumentar épocas, añadir neuronas o ajustar la tasa de aprendizaje |\n","| Curvas inestables | Entrenamiento irregular | Ajustar `batch_size` o normalizar los datos |"],"metadata":{"id":"i-LGcMGCPfA6"}},{"cell_type":"markdown","source":["## 1.5 Fase de mejora\n","\n","Una vez evaluado el modelo, el siguiente paso consiste en mejorar su rendimiento y capacidad de generalización.\n","Esta fase busca evitar que el modelo memorice los datos de entrenamiento (overfitting) y ajustar sus hiperparámetros para obtener la mejor configuración posible."],"metadata":{"id":"VVWlMY-VQ4Ws"}},{"cell_type":"markdown","source":["### <b>1.5.1 Prevención del Overfitting</b> *Regularización*\n","El overfitting se produce cuando el modelo aprende demasiado los datos de entrenamiento, incluyendo su ruido, y pierde capacidad para generalizar a nuevos datos.\n","\n","En esta situación, el error de entrenamiento (`loss`) sigue bajando mientras el error de validación (`val_loss`) empieza a subir.\n","\n","Las principales técnicas aplicadas para prevenirlo son las siguientes:"],"metadata":{"id":"B4nveStZRUGQ"}},{"cell_type":"markdown","source":["#### a) <b>Reducción del `batch_size`</b>\n","\n","Reducir el tamaño del *batch* introduce más variabilidad (ruido) en las actualizaciones de los pesos, lo que mejora la capacidad de generalziación y reduce el sobreajuste.\n","\n","```python\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=50,\n","    batch_size=16,        # Tamaño de lote reducido\n","    validation_split=0.2,\n","    verbose=1\n",")\n","```\n","\n","Cuándo usarlo:\n","Cuando el modelo sobreajusta y las curvas loss y val_loss se separan mucho.\n","\n","#### <b> b) Dropout </b>\n","\n","El Dropout \"apaga\" aleatoriamente un porcentaje de neuronas durante el entrenamiento. Esto impide que el modelo dependa excesivamente de conexiones específicas y mejora su capacidad de generalización.\n","\n","```python\n","from tensorflow.keras import layers, models\n","\n","model = models.Sequential([\n","    layers.Input(shape=(10,)),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.3),  # desactiva el 30% de las neuronas\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(1, activation='sigmoid')\n","])\n","```\n","\n","Cuándo usarlo:\n","Cuando el modelo muestra buen rendimiento en entrenamiento pero bajo en validación.\n","\n","#### <b>c) Batch Normalization </b>\n","\n","La Batch Normalization normaliza las activaciones intermedias dentro del modelo, lo que estabiliza el entrenamiento y permite usar tasas de aprendizaje más altas sin divergencias.\n","\n","\n","```python\n","from tensorflow.keras import layers, models\n","\n","model = models.Sequential([\n","    layers.Input(shape=(10,)),\n","    layers.Dense(128, activation='relu'),\n","    layers.BatchNormalization(),  # normaliza las activaciones\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(1, activation='sigmoid')\n","])\n","```\n","\n","Cuándo usarlo:\n","Cuando el entrenamiento es inestable o la pérdida oscila entre épocas.\n","\n","#### <b> d) Layer Normalization</b>\n"," La Layer Normalization normaliza las activaciones a nivel de muestra (no por batch). Es útil cuando el tamaño de batch es pequeño o variable.\n","\n","```python\n","from tensorflow.keras.layers import LayerNormalization\n","\n","model = models.Sequential([\n","    layers.Input(shape=(10,)),\n","    layers.Dense(128, activation='relu'),\n","    LayerNormalization(),  # normalización por muestra\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(1, activation='sigmoid')\n","])\n","```\n","\n","Cuándo usarlo:\n","Cuando el tamaño del batch es muy pequeño o la red presenta inestabilidad (las métricas (como la pérdida loss o la precisión accuracy) suben y bajan de forma irregular).\n","\n","#### <b> e) Regularización L1, L2 y Elastic Net</b>\n","\n","Las téncias de regularización penalizan los pesos grandes añadiendo un término a la fundión de pérdida. Esto ayuda a mantener el modelo más simple y menos propenso al overfitting.\n","\n","```python\n","from tensorflow.keras import regularizers\n","\n","# Regularización L1\n","layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001))\n","\n","# Regularización L2\n","layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))\n","\n","# Regularización Elastic Net (L1 + L2)\n","layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))\n","```\n","\n","Cuándo usarlo:\n","Cuando el modelo tiene muchos parámetros o hay riesgo de sobreajuste.\n","\n","#### <b> f) Early Stopping </b>\n","\n","El Early Stopping es una téncica que detiene automáticamente el entrenamiento cuando el modelo deja de mejorar el conjunto de validación. Su oobjetivo es evitar el sobreajuste y ahorrar tiempo de entrenamiento innecesario.\n","\n","*Funcionamiento*: Durante el entrenamiento, Keras monitoriza una métrica (por ejemplo, val_loss).\n","Si esa métrica no mejora después de varias épocas consecutivas, el entrenamiento se detiene y, opcionalmente, se restauran los pesos del mejor punto alcanzado.\n","\n","```python\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Definimos el callback de Early Stopping\n","early_stop = EarlyStopping(\n","    monitor='val_loss',         # métrica a observar\n","    patience=5,                 # nº de épocas sin mejora antes de detener\n","    restore_best_weights=True,  # vuelve a los mejores pesos\n","    verbose=1\n",")\n","\n","# Entrenamiento con Early Stopping\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=100,\n","    batch_size=32,\n","    validation_split=0.2,\n","    callbacks=[early_stop],\n","    verbose=1\n",")\n","```\n","\n","Parametros princiaples:\n","\n","* `monitor`:Métrica a vigilar (``val_loss``, ``val_accuracy``, etc.)\n","* `patience`: Número de épocas que puede pasar sin mejora antes de parar\n","* `restore_best_weights`: Si es ``True``, recupera automáticamente los pesos con mejor rendimiento\n","* `mode`: Determina si se busca minimizar o maximizar la métrica (``min`` o ``max``)"],"metadata":{"id":"v3tDTjnVryG8"}},{"cell_type":"markdown","source":["### <b>1.5.2 Ajuste de Hiperparámetros (Hyperparameter Tunning) </b> *Optimización*\n","Los hiperparámetros son valores que determinan cómo se entrena el modelo y no se aprenden automáticamente.\n","\n","Ejemplos: número de neuronas, tasa de aprendizaje, número de capas, función de activación, etc.\n","\n","Ajustarlos correctamente puede marcar una gran diferencia en el rendimiento del modelo.\n","\n","Técnicas de ajuste:"],"metadata":{"id":"icsG3s-0uVcs"}},{"cell_type":"markdown","source":["#### <b>a) Búsqueda manual </b>\n","\n","Consiste en probar diferentes configuraciones y observar el rendimiento del modelo en valdiación. Es la forma más simple pero también la más lenta.\n","\n","Cuándo usarlo:\n","Para exploraciones rápidas o cuando los recursos de cómputo son limitados.\n","\n","#### <b> b) Ajuste automático con Keras Tuner </b>\n","\n","La librería Keras Tuner permite explorar automáticamente diferentes combinaciones de hiperparámetros y encontrar la configuración óptima.\n","\n","Entendiendo las diferentes estrategias de tuneo:\n","\n","Keras Tuner nos ofrece 4 principales tecnicas de optimización de hiperparámetros:\n","\n","1. RandomSearch\n","    * Como funciona: Selecciona aleatoriamente muestras del espacio de hiperarámetros\n","    * Pros: Simple, fácilmente paralelizable, no hace suposiciones sobre la importancia de los parámetros\n","    * Cons: Puede ser ineficiente en espacios de búsqueda grandes.\n","    * Mejor para: exploración incial o cuando se sabe pcoo sobre el espacio de hiperparámetros.\n","\n","2. Hyperband\n","    * Como funciona: Asigna recursos (epochs) de forma dinámica, descartando rápidamente los modelos con bajo rendimiento.\n","    * Pros: Más eficiente que la búsqueda aleatoria, especialmente para redes profundas\n","    * Cons: más complejo de configurar correctamente.\n","    * Mejor para: Cuando el entrenamiento es computacionalmente costoso y se quiere equilibrarexploración vs explotación.\n","\n","3. BayesianOptimizatipon\n","    * Como funciona: construe un modelo de probabilidad de la función objetivo y lo usa para seleccionar hiperparámetros\n","    * Pros: uso más eiciente de los recursos, aprende de evaluaciones previas.\n","    * Cons: más compleo, requiere mayor costo computacional en cada iteración\n","    * Mejor para: cuando la evaluación es costosa y se tiene un espacio de búsqueda moderado.\n","\n","4. Sklearn\n","    * Como funciona: interfaz para los métodos de búsqueda de hiperparámetros de scikit-learn.\n","    * Pros: API familiar para quienes provienen de scikit-learn.\n","    * Cons: limitado a las capacidades de ajuste de hiperparámetors de scikit-learn.\n","    * Mejor para: cuando se integra con pipelines existentes de scikit-learn.\n","\n","```python\n","import keras_tuner as kt\n","from tensorflow.keras import layers, models\n","\n","# Definición del modelo adaptable\n","def build_model(hp):\n","    model = models.Sequential()\n","    model.add(layers.Input(shape=(10,)))\n","    \n","    # Número de unidades variable\n","    model.add(layers.Dense(\n","        units=hp.Int('units', min_value=32, max_value=256, step=32),\n","        activation='relu'\n","    ))\n","    \n","    # Tasa de aprendizaje variable\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","        loss='mse',\n","        metrics=['mae']\n","    )\n","  return model\n","\n","# Búsqueda aleatoria de combinaciones\n","tuner = kt.RandomSearch(\n","    build_model,\n","    objective='val_mae',\n","    max_trials=5,\n","    directory='tuning_results',\n","    project_name='Regression_Tuning'\n",")\n","\n","# Ejecución del tuning\n","tuner.search(x_train, y_train, epochs=20, validation_split=0.2)\n","\n","```"],"metadata":{"id":"tfX0CXoivuqv"}},{"cell_type":"markdown","source":["###<b> 1.5.3. Capas de Embedding: Representación de Variables Categóricas</b>\n","\n","\n","En las redes densas (ANN), la capa de Embedding se utiliza para transformar datos categóricos en un espacio continuo de baja dimensión. Es una alternativa superior al *One-Hot Encoding* cuando trabajamos con categorías de alta cardinalidad.\n","\n","\n","<b>Parámetros de la Capa y su Configuración</b>\n","\n","Para definir la capa layers.Embedding(input_dim, output_dim), debemos entender qué papel juega cada número:\n","\n","1. `input_dim` (Rango de Entrada):\n","    * Para qué sirve: Define cuántas categorías únicas existen en esa variable.\n","    * Cómo se define: Es el valor del número entero más alto que la red recibirá + 1.\n","    * Ejemplo: Si tienes 100 tipos de productos (indexados del 0 al 99), `input_dim = 100`.\n","\n","2. `output_dim` (Capacidad de Representación):\n","    * Para qué sirve: Define el tamaño del vector (cuántas neuronas representan a cada categoría).\n","    * Cómo se define: Es un hiperparámetro de arquitectura.\n","      * Un valor pequeño (ej. 4-8) captura relaciones simples.\n","      * Un valor alto (ej. 32-64) permite capturar relaciones complejas, pero aumenta el riesgo de Overfitting.\n","    * Regla de oro: Se suele usar la raíz cuarta del número de categorías ($\\sqrt[4]{input\\_dim}$) como punto de partida.\n","\n","\n","<b>Implementación en la Estructura ANN</b>\n","\n","En las ANN, el embedding debe ser la primera capa de la rama que procesa los datos categóricos y siempre debe ir seguido de un Flatten para poder conectar con las capas Dense.\n","\n","```python\n","from tensorflow.keras import layers, models\n","\n","# Configuración de parámetros según el dataset\n","total_categorias = 200    # input_dim (ej. 200 ciudades diferentes)\n","dimension_vector = 10     # output_dim (cada ciudad será un vector de 10 números)\n","\n","model = models.Sequential([\n","    # Recibe el índice entero de la categoría\n","    layers.Input(shape=(1,)),\n","    \n","    # Crea la tabla de búsqueda (Embedding)\n","    layers.Embedding(input_dim=total_categorias,\n","                     output_dim=dimension_vector,\n","                     name=\"capa_embedding\"),\n","    \n","    # Convierte la salida del embedding (1, 10) en un vector plano (10,)\n","    layers.Flatten(),\n","    \n","    # Capas densas de la ANN\n","    layers.Dense(64, activation='relu'),\n","    layers.Dropout(0.3),  # Técnica de mejora contra overfitting\n","    layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy')\n","```"],"metadata":{"id":"6rCUPGmPfHQm"}},{"cell_type":"markdown","source":["## 1.6. Fase de producción\n","\n"],"metadata":{"id":"IjJT4V_q0zSI"}},{"cell_type":"markdown","source":["# <b> 2. Convolutional Neural Network (CNN) model in Keras </b>\n","\n","Una Red Neuronal Convolucional (CNN) es uun tipo de red neuronal diseñada para trabajar con datos que presentan estructura espacial, como imágenes o señales. Emplea filtros (kernales) que se deslizan sobre la entrada para extraer automáticmanete características relevantes, desde patrones simples hasta representaciones más complejas. Se utiliza principalmente en tareas de clasificación y reconocimiento de <b>imágenes</b>, detección, segmentación y, en general <b>visión por computador</b>.\n","\n","#### <b> Estrucutra básica de una red neuronal convolucional </b> ####\n","\n","Una arquitectura de red neuronal convolucional consta de las siguientes partes principales:\n","\n","* Capa de entrada (Input Layer): recibe el tensor de los datos de entrada (por ejemplo, una imagen con dimesniones alto x ancho x canales).\n","\n","* Capas concolucionales (Convolutional Layers): aplican filtros que extraen características locales (bordes, texturas, formas), generando mapas de características.\n","\n","* Capas de reducción espacial (Pooling Layers): reducen la dimensión espacial de los mapas de características para disminuir parámetros y controalr el sobreajuste.\n","\n","* Capa de aplanado o agregación global (FLatten / GlobalAveragePolling): transforma los mapas de características en un vector de características o realiza un promedio global por canal.\n","\n","* Capas Densas (Fully connected Layers): combina las características extraídas para aprender relaciones de mayor nivel.\n","\n","* Capa de salida (Output layer): Produce la predicción final."],"metadata":{"id":"qZgrZyL3c2iv"}},{"cell_type":"markdown","source":["## Librerías necesarias\n","\n","Antes de comenzar a construir una red neuronal, debemos importar las librerías esenciales para definir, entrenar y analizar el modelo. Estas librerías cubren distintos aspectos: modelado, preprocesamiento, visualización y optimización."],"metadata":{"id":"HA4iWuskzeHY"}},{"cell_type":"code","source":["# TensorFlow y Keras: construcción y entrenamiento del modelo\n","import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator  # para data augmentation\n","\n","# NumPy: operaciones matemáticas y manejo de arreglos numéricos\n","import numpy as np\n","\n","# Matplotlib: visualización de imágenes, resultados y métricas\n","import matplotlib.pyplot as plt\n","\n","# Scikit-learn: métricas adicionales de evaluación (precisión, matriz de confusión, etc.)\n","from sklearn.metrics import classification_report, confusion_matrix"],"metadata":{"id":"Z6Kf4ahVz0nr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.1 Fase de construcción\n","\n","En esta fase se define la arquitectura de la red neuronal convolucional, es decir, la secuencia de capas que la componen y la forma en la que se conectan entre sí. Cada capa tiene una función específica dentro del modelo, y juntas permiten que la red aprenda las características más relefantes de los datos.\n","\n","A continuación se describen las capas mas counes utilizadas en la construcción de una CNN."],"metadata":{"id":"70BBrpXA0BPp"}},{"cell_type":"markdown","source":["### <b> 2.1.1. Capa de entrada (Input Layer) </b>\n","\n","(NOTA: Esta capa no utiliza función de activación)\n","\n","La capa de entrada define la forma de los datos que recibe el modelo. En las CNN, los datos de entrada se representan como tensores tridimensionales con las dimensiones (alto,ancho,canales).\n","\n","Por ejemplo, una imagen RGB de 32x32 píeles tiene forma de (32,32,3).\n","\n","Parámetros principales:\n","\n","* shaepe: dimensiones del tensor de entrada (alto,ancho,canales).\n","* name: nomdre identificador de la capa (opcional).\n","\n","API Funcional\n","\n","```python\n","inputs = layers.Input(shape=(32, 32, 3), name='input_layer')\n","````\n","\n","API Secuencial\n","\n","```python\n","model = models.Sequential()\n","model.add(layers.Input(shape=(32, 32, 3), name='input_layer'))\n","````\n"],"metadata":{"id":"37kPkv0W0Z23"}},{"cell_type":"markdown","source":["### <b> 2.1.2 Capas Convolucionales (Convolutional Layers) </b>\n","\n","Las capas convolucionales son el núclo de una CNN.\n","\n","Aplican filtros (kernels) que se desplazan sobre la imagen para extraer características locals como bordes, texturas o formas.\n","\n","Cada filtro genera un mapa de características (feature map) que resalta la presencia de un patrín concreto.\n","\n","<b>Parámetros principales:</b>\n","\n","* `filters:` número de filtros o mapas de características.\n","* `kernel_size:` tamaño del filtro (por ejemplo, (3,3)).\n","* `strides:` paso de desplazamiento del filtro.\n","* `padding:` ``valid`` (sin relleno) o ``same`` (mantiene el tamaño)\n","* `activation:` función de activación utilizada (por ejemplo, ``relu``).\n","* `name:` nombre de la capa (opcional).\n","\n","API Funcional\n","\n","```python\n","x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu',\n","                  padding='same', name='conv_1')(inputs)\n","x = layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu',\n","                  padding='same', name='conv_2')(x)\n","````\n","\n","API Secuencial\n","\n","```python\n","model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same', name='conv_1'))\n","model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same', name='conv_2'))\n","````\n","\n","<b>Funciones de activación </b>\n","\n","* `ReLU`: la más utilizada en CNN. Ideal para clasificación de imágenes generales (por ejemplo, CIFAR-10 o MNIST).\n","* `LeakyReLU`: alternativa cuando hay riesgo de que muchas neuronas queden inactivas (problema de “ReLU muerta”).\n","* `ELU`: útil en modelos más profundos, mejora la convergencia manteniendo valores negativos pequeño\n","* `Tanh`: poco común en CNN modernas, pero útil si los datos están centrados entre -1 y 1 (por ejemplo, imágenes normalizadas).\n"],"metadata":{"id":"OL45IIHCSa6I"}},{"cell_type":"markdown","source":["### <b>2.1.3 Capas de reducción espacial (Polling Layers)</b>\n","(NOTA: estas capas no utilizan función de activación)\n","\n","Las capas de *pooling* reducen el tamaño espacial de los mapas de características, conservando la información más importante.\n","\n","Esto disminuye la complejidad computacional y ayuda a evitar el sobreajuste.\n","\n","<b>Tipos más comunes:</b>\n","\n","* `MaxPooling2D`: selecciona el valor máximo en cada región (más iusado en clasificación).\n","* `AveragePooling2D`: calcula el promedio de los valores (útil cuando se busca suavizar ruido).\n","\n","<b>Parámetros princiaples:</b>\n","\n","* `pool_size`:= tamaño de la ventana (por ejemplo, (2,2))\n","* `strides`: desplazamiento (por defecto igual a `pool_size`).\n","* `padding`: 'valid' o 'same'.\n","* `name`: nombre de la capa.\n","\n","API Funcional\n","\n","```python\n","x = layers.MaxPooling2D(pool_size=(2,2), name='pool_1')(x)\n","````\n","\n","API Secuencial\n","\n","```python\n","model.add(layers.MaxPooling2D((2,2), name='pool_1'))\n","````\n","\n"],"metadata":{"id":"63xFOdqMD1MV"}},{"cell_type":"markdown","source":["### <b> 2.1.4. Capa de aplanado (Flatten Layer</b>\n","(NOTA: esta capa no utiliza función de activación)\n","\n","Convierte los mapas de característicasbidimensionales en un vector unidminensional, para conectar las capas conolucionales con las capas desas finales.\n","\n","API Funcional\n","\n","```python\n","x = layers.Flatten(name='flatten')(x)\n","````\n","\n","API Secuencual\n","\n","```python\n","model.add(layers.Flatten(name='flatten'))\n","````\n"],"metadata":{"id":"cjUKD-t1I_we"}},{"cell_type":"markdown","source":["### <b>2.1.5. Capas densas (Fully Connected Layers) </b>\n","\n","las capas densas combinan las características extraídas por las capas convolucionales.\n","Cada neurona se cpnecta con toas las neuronas de la capa siguiente, lo que permite al modelo aprender relaciones de alto nivel.\n","\n","<b>Parámetros principales</b>\n","\n","* `units`: número de neuronas\n","* `activation`: función de activación utilizada\n","* `name`: nombre de la capa\n","\n","API Funcional\n","\n","```python\n","x = layers.Dense(units=64, activation='relu', name='dense_1')(x)\n","````\n","\n","API Secuencial\n","\n","```python\n","model.add(layers.Dense(64, activation='relu', name='dense_1'))\n","````\n","\n","<b>Funciones de activación</b>\n","\n","* `ReLU`: opción general para capas ocultas intermedias. Ejemplo: clasificación de imágenes en varias clases.\n","* `Tanh`: útil cuando los datos o las salidas intermedias están centrados en 0 (por ejemplo, embeddings normalizados).\n","* `Sigmoid`: rara en capas ocultas, pero útil si se desea una salida intermedia entre 0 y 1 (por ejemplo, probabilidad parcial en modelos autoencoder).\n","* `ELU`: recomendable en modelos más profundos donde ReLU pueda saturarse."],"metadata":{"id":"GDDnT-wTJ1s4"}},{"cell_type":"markdown","source":["### <b>2.1.6. Capa de salida (Ouput Layer)</b>\n","\n","la capa de salida genera la predicción final del modelo.\n","\n","El número de neuronas y la función de activación dependen del tipo de problema que se quiera resolver.\n","\n","<b>Funciones de activación </b>\n","\n","\n","| **Tipo de tarea**              | **Ejemplo**                                 | **Activación**             | **Cuándo usarla**                                                                 |\n","|--------------------------------|---------------------------------------------|-----------------------------|----------------------------------------------------------------------------------|\n","| **Clasificación binaria**      | Imagen con o sin gato                       | `Sigmoid`                  | Cuando hay solo dos clases posibles. Devuelve una probabilidad entre 0 y 1.     |\n","| **Clasificación multiclase**   | CIFAR-10 (10 categorías)                    | `Softmax`                  | Cuando las clases son excluyentes (una sola categoría por imagen).              |\n","| **Clasificación multietiqueta**| Imagen con varias etiquetas posibles        | `Sigmoid` *(una por clase)* | Cuando una imagen puede pertenecer a más de una clase (por ejemplo, “perro” y “exterior”). |\n","| **Regresión**                  | Predicción de coordenadas o valores continuos| `Linear / Tanh`            | `Linear` si los valores no tienen límite; `Tanh` si están normalizados en [-1,1]. |\n","| **Reconstrucción o generación**| Autoencoders o GANs                         | `Sigmoid / Tanh`           | Según el rango de valores de los píxeles de salida (`[0,1]` o `[-1,1]`).        |\n","\n","\n","<b>Clasificación binaria (`Sigmoid`)</b>\n","\n","API Funcional\n","\n","```python\n","outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n","````\n","\n","API Secuencial\n","\n","```python\n","layers.Dense(1, activation='sigmoid', name='output')\n","````\n","\n","<b>Clasificación multiclase (`Softmax`)</b>\n","\n","API Funcional\n","\n","```python\n","outputs = layers.Dense(10, activation='softmax', name='output')(x)\n","````\n","\n","API Secuencial\n","\n","```python\n","    layers.Dense(10, activation='softmax', name='output')\n","````\n","\n","<B>Clasificación multietiqueta (`Sigmoid` por clase)</B>\n","\n","API Funcional\n","\n","```python\n","outputs = layers.Dense(n_labels, activation='sigmoid', name='output')(x)\n","````\n","\n","API Secuencial\n","\n","```python\n","    layers.Dense(n_labels, activation='sigmoid', name='output')\n","````\n","<b>Regresión (``Linear`` o ``Tanh``) </b>\n","\n","API Funcional\n","\n","```python\n","outputs = layers.Dense(1, activation='linear', name='output')(x)\n","````\n","\n","API Secuencial\n","\n","```python\n","    layers.Dense(1, activation='linear', name='output')\n","````"],"metadata":{"id":"N97hz6zpMJoF"}},{"cell_type":"markdown","source":["### <b>2.1.7. Construcción completo del model (ejemplo CIFAR-10)</b>\n","\n","Consturcción de un modelo CIFAR-10, es decir un problema de clasificación de multiclase de imágenes.\n","\n","API Funcional\n","\n","```python\n","inputs = layers.Input(shape=(32, 32, 3), name='input')\n","x = layers.Conv2D(32, (3,3), activation='relu', padding='same', name='conv_1')(inputs)\n","x = layers.MaxPooling2D((2,2), name='pool_1')(x)\n","x = layers.Conv2D(64, (3,3), activation='relu', padding='same', name='conv_2')(x)\n","x = layers.MaxPooling2D((2,2), name='pool_2')(x)\n","x = layers.Flatten(name='flatten')(x)\n","x = layers.Dense(64, activation='relu', name='dense_1')(x)\n","outputs = layers.Dense(10, activation='softmax', name='output')(x)\n","\n","model = models.Model(inputs=inputs, outputs=outputs, name='CNN_model')\n","model.summary()````\n","\n","API Secuencial\n","\n","```python\n","model = models.Sequential([\n","    layers.Input(shape=(32, 32, 3), name='input'),\n","    layers.Conv2D(32, (3,3), activation='relu', padding='same', name='conv_1'),\n","    layers.MaxPooling2D((2,2), name='pool_1'),\n","    layers.Conv2D(64, (3,3), activation='relu', padding='same', name='conv_2'),\n","    layers.MaxPooling2D((2,2), name='pool_2'),\n","    layers.Flatten(name='flatten'),\n","    layers.Dense(64, activation='relu', name='dense_1'),\n","    layers.Dense(10, activation='softmax', name='output')\n","])\n","\n","model.summary()````"],"metadata":{"id":"1zuqzqwnQCNv"}},{"cell_type":"markdown","source":["## 2.2. Fase de Compilación\n","\n","Una vez construida la arquitectura de la red neuronal (definidas las capas, sus funciones de activación y conexiones), el siguiente pasi es compilar el modelo.\n","\n","Durante la compilación, se especifica cómo el modelo va a aprender. Para ello, sedefinen tres elementos esenciales:\n","\n","1. La función de pérdida (loss): indica qué mide y qué debe minimizar.\n","2. El optimizador (optimizier): indica cómo se actualizan los pesos para reducr la pérdida.\n","3. Las métricas (metrisc). sirven para monitorizar el rendimiento durante el entrenamiento, pero no afectan al aprendizaje."],"metadata":{"id":"oYmWJkwCRJAH"}},{"cell_type":"markdown","source":["###<b> Función de pérdida (loss) </b>\n","\n","la función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. El objetivo del entrenamiento es minimizar esta función.\n","\n","Dependiendo del tipo de problema, se usa una función diferente:\n","\n","<b>Clasificación binari </b>\n","\n","* Activación de salida: ``sigmoid``\n","* Función de pérdida: ``binary_crossentropy``\n","* ej.: “gato” vs “no gato”\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","```\n","\n","<b>Clasificación multiclase</b>\n","\n","* Activación de salida: ``softmax``\n","\n","* Pérdida:\n","    * ``categorical_crossentropy`` (si las etiquetas están codificadas en one-hot)\n","    * ``sparse_categorical_crossentropy`` (si las etiquetas son enteras)\n","* ej.: CIFAR-10: 10 clases exclusivas\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='categorical_crossentropy',  # o 'sparse_categorical_crossentropy'\n","    metrics=['accuracy']\n",")\n","```\n","\n","<b>Clasificación multietiqueta</b>\n","\n","* Activación de salida: ``sigmoid`` (una por clase)\n","\n","* Función de pérdida: ``binary_crossentropy`` (aplicada de forma independiente a cada etiqueta)\n","* ej.: etiquetas no excluyentes: “perro”, “exterior”, “noche”…\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","```\n","\n","<b>Regresión</b>\n","\n","* Activación de salida: ``linear``\n","\n","* Pérdida:\n","    * ``mse`` (mean squared error – error cuadrático medio)\n","    * ``mae`` (mean absolute error – error absoluto medio)\n","* ej.: predecir un valor continuo: brillo, edad, ángulo…\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='mse',       # o 'mae'\n","    metrics=['mae']\n",")\n","```\n"],"metadata":{"id":"AiuHr0lSUL7h"}},{"cell_type":"markdown","source":["### <b> Optimizador (optimizer) </b>\n","\n","El optimizador define cómo se ajustan los pesos del modelo en cada interación.\n","\n","Su función es minimizar la pérdida mediante el descenso del gradiente o métodos adaptattivos.\n","\n","Las más comunes son:\n","\n","``Adam`` (Adaptive Moment Estimation)\n","\n","* Método adaptativo, rápido y estable.\n","* Ajusta la tasa de aprendizaje de forma individual para cada parámetro.\n","* Recomendado por defecto para la mayoría de CNN.\n","\n","```python\n","model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","```\n","``RMSprop``\n","\n","* Variante de gradient descent adaptativa.\n","* Ideal para redes con datos no estacionarios (por ejemplo, secuencias o video).\n","* Controla la variabilidad de los gradientes.\n","\n","```python\n","model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","\n","```\n","\n","``SGD`` (Stochastic Gradient Descent)\n","\n","* Gradiente descendente estocástico.\n","* Actualiza los pesos lentamente, con posibilidad de usar momentum.\n","* Ideal cuando se busca un entrenamiento controlado y estable.\n","\n","```python\n","model.compile(optimizer='sgd', loss='mse', metrics=['mae'])\n","```\n"],"metadata":{"id":"gGXgnQ9rYU-Z"}},{"cell_type":"markdown","source":["### <b>Métricas (metrics)</b>\n","\n","Las métricas permiten evaluar el rendimeinto del modelo durante el entrenamiento y la validación. No influyen en el proceso de optimización, pero sirven como referencia para analizar el desempeño.\n","\n","``Accuracy``\n","* Tipo de problema: Clasificación binaria o multiclase.\n","* Descripción: Mide el porcentaje de aciertos sobre el total de predicciones.\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","```\n","\n","``mae (Mean Absolute Error)``\n","* Tipo de problema: Regresión.\n","* Descripción: Calcula el promedio del valor absoluto de los errores.\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='mse',      # pérdida puede ser MSE\n","    metrics=['mae']  # seguimiento del error medio absoluto\n",")\n","```\n","\n","``mse (Mean Squared Error)``\n","* Tipo de problema: Regresión.\n","* Descripción: Calcula el promedio de los errores al cuadrado (penaliza más los errores grandes)\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='mse',      # pérdida igual a la métrica\n","    metrics=['mse']\n",")\n","```\n"],"metadata":{"id":"E600SqL1doXp"}},{"cell_type":"markdown","source":["## 2.3. Fase de entrenamiento\n","\n","Una vez compilado el modelo, se procede a entrenarlo.\n","\n","Durante esta fase, el modelo ajusta sus pesos internos para minimizar la función de péridad definda en la compilación y mejorar las métricas.\n","\n","El enternamiento se realiza con el método `fit()`, que recibe los atos, el número de épocas, el tamaño de los lotes y, opcionalmente, un conjunto de validación.\n"],"metadata":{"id":"ibWa77yQ5f8Y"}},{"cell_type":"markdown","source":["\n","<b> Parámetros del método </b>\n","\n","* ``x, y``: datos de entrada y etiquetas verdaderas.\n","\n","* ``epochs``: número de pasadas completas sobre el conjunto de entrenamiento.\n","\n","* ``batch_size``: tamaño del lote (muestras procesadas antes de actualizar pesos).\n","\n","* ``validation_data``: tupla (x_val, y_val) para evaluar al final de cada época.\n","\n","* ``validation_split``: fracción de x/y usada automáticamente como validación (si no se pasa validation_data).\n","\n","* ``verbose``: nivel de detalle (0: silencioso, 1: barra de progreso, 2: por época).\n","\n","<b> Entrenamiento (clasificación – CNN con CIFAR-10) </b>\n","\n","```python\n","history = model.fit(\n","    x_train, y_train,                 # Datos de entrenamiento\n","    epochs=20,                        # Número de épocas\n","    batch_size=64,                    # Tamaño del lote\n","    validation_data=(x_val, y_val),   # Conjunto de validación\n","    verbose=1                         # Mostrar progreso\n",")\n","```\n","\n","* uso típico con saldia `softmax` y pérdida `categorical_crossentropy`/`spare_categorical_crossentropy`\n","* `history` almacena la evolución de loss y métricas por época (incluyendo las de validación si se proporcionan).\n","\n","\n","<b> Entrenamiento (regresión – predicción continua) </b>\n","\n","```python\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=50,\n","    batch_size=32,\n","    validation_split=0.2,  # Reserva automáticamente el 20% para validación\n","    verbose=1\n",")\n","```\n","\n","* Uso típico con salida `linear` y pérdidas de regresión (`mse` o `mae`).\n","\n","* `validation_split` es útil cuando no se dispone de un conjunto de validación separado."],"metadata":{"id":"IOMieFvjCbbB"}},{"cell_type":"markdown","source":["## 2.4. Fase de Evaluación\n","\n","Tras el entrenamiento, se evalúa el modelo para medir su rendimiento en datos no vistos y analizar su comportamiento (aciertos/errores).\n","\n","La evaluación se realioza con:\n","\n","* `model.evaluate()`: calcula la pérdida y las métricas en el conjunto de test.\n","* `model.predict()`: obtiene las probabilidades/logits para analizar predicciones (por ejemplo, clases previstas).\n","\n","* Gráficas de evolución (a partir de `history`): ayudan a revisar cómo ha aprendido el modelo,\n","* Inspección de errores: visualziar ejemplos mal clasificados con sus probabilidades.\n","\n"],"metadata":{"id":"LK0CJ0B2-F63"}},{"cell_type":"markdown","source":["\n","<b> Evolución cuantitativa con `evaluate()` </b>\n","\n","```python\n","# Evalúa en el conjunto de test (p. ej., CIFAR-10)\n","results = model.evaluate(X_test_norm, y_test, verbose=1)\n","```\n","* results[0] → pérdida en test.\n","\n","* results[1] → métrica principal (por ejemplo, accuracy).\n","\n","<b> Prediccions con `predict()` y clases previstas </b>\n","\n","```python\n","# Probabilidades/logits para cada imagen\n","predictions = model.predict(test_images)\n","\n","# Clases previstas (si la salida es softmax/multiclase)\n","predicted_classes = np.argmax(predictions, axis=-1)\n","```\n","* Útil para analziar casos concretos, construir informes o generar visualizaciones\n","\n","\n","<b> Gráficas de entrenamiento (pérdida y precisión) </b>\n","\n","Estas gráficas se generan a partir del objeto `history` devuelto por `model.fit()`.\n","Sirven para evaluar el proceso de aprendizaje (sesgo/varianza, sobreajuste, etc.).\n","\n","```python\n","import pandas as pd\n","def show_loss_accuracy_evolution(history):\n","    # Convierte el historial a DataFrame y añade el índice de época\n","    hist = pd.DataFrame(history.history)\n","    hist['epoch'] = history.epoch\n","\n","    # Dos subgráficos: pérdida y precisión\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n","\n","    # Pérdida\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","    ax1.plot(hist['epoch'], hist['loss'], label='Train Loss')\n","    ax1.plot(hist['epoch'], hist['val_loss'], label='Val Loss')\n","    ax1.grid()\n","    ax1.legend()\n","\n","    # Precisión (si está disponible en history)\n","    if 'accuracy' in hist and 'val_accuracy' in hist:\n","        ax2.set_xlabel('Epoch')\n","        ax2.set_ylabel('Accuracy')\n","        ax2.plot(hist['epoch'], hist['accuracy'], label='Train Acc')\n","        ax2.plot(hist['epoch'], hist['val_accuracy'], label='Val Acc')\n","        ax2.grid()\n","        ax2.legend()\n","    plt.show()\n","\n","```\n","\n","Lectura rápida de las curvas:\n","\n","* Paralelas y descendentes (loss) con val_loss similar → buen aprendizaje.\n","* Train loss ↓ pero val_loss ↑ → posible sobreajuste (overfitting).\n","\n","\n","<b> Inspección cualitativa de errores </b>\n","\n","Visualiza ejemplos del conjunto de validación mal clasificados, mostrando la clase predicha, la clase real y sus probabilidades.\n","(Requiere un `val_ds` de tipo `tf.data.Dataset` y `class_names_list` con los nombres de las clases).\n","\n","```python\n","def show_errors(val_ds, model, class_names_list, n_images=10):\n","    n_plots = 0\n","    for images, labels in val_ds:\n","        # Predicciones para este batch\n","        pred_probs = model.predict(images)\n","        pred_classes = np.argmax(pred_probs, axis=-1)\n","\n","        # Compara con etiquetas reales\n","        for ind in range(len(images)):\n","            if n_plots >= n_images:\n","                return\n","\n","            real_idx = labels[ind].numpy()\n","            pred_idx = pred_classes[ind]\n","\n","            # Si hay error, lo mostramos\n","            if pred_idx != real_idx:\n","                pred_class = class_names_list[pred_idx]\n","                real_class = class_names_list[real_idx]\n","\n","                prob_pred = float(np.max(pred_probs[ind]))\n","                prob_real = float(pred_probs[ind][real_idx])\n","\n","                plt.imshow(images[ind].numpy().astype(\"uint8\"))\n","                plt.title(\n","                    f\"Predicted: {pred_class}, prob: {prob_pred:.2f}\\n\"\n","                    f\"Real: {real_class}, prob: {prob_real:.2f}\"\n","                )\n","                plt.axis('off')\n","                plt.show()\n","                n_plots += 1\n","\n","```\n","\n","Utilidad:\n","\n","* Detectar patrones de error (clases confundidas, fondos complejos, iluminación, etc.).\n","\n","* Guiar decisiones en la Fase de Mejora (data augmentation específico, arquitectura, regularización)"],"metadata":{"id":"Zkq81VoyCg5R"}},{"cell_type":"markdown","source":["## 2.5. Fase de mejora\n","\n","Una vez evaluado el modelo de red neuronal convolucional (CNN), el siguiente paso consiste en optimizar su rendimiento y capacidad de generalización.\n","\n","Esta fase busca reducir el sobreajuste (overfitting), mejorar la precisión en los datos de validación y ajusta la arquitectura y los hiperparámetros de las capas convolucionales para obtener el mejor desempeño posible.\n"],"metadata":{"id":"3rnB6r2OCNdZ"}},{"cell_type":"markdown","source":["### <b> 2.5.1. Prevención del Overfitting</b> *Regularización*\n","\n","El overfitting ocurre cuando la CNN aprende demsiado los detalles y el ruido de las imágenes de entrenamiento, perdiendo capacidad de generalizar nuevas imagenes.\n","\n","En este caso, la loss de entrenamiento continúa disminuyendo mientras la val_loss comienza a aumentar.\n","\n","Las principales técnicas aplicadas para prevenirlo son las siguientes:\n","\n"],"metadata":{"id":"BwMC8MBQWCyZ"}},{"cell_type":"markdown","source":["<b>a) Data Augmentation (Aumento de datos) </b>\n","\n","El Data Augmentation genera nuevas imágenes a partir de las originales mediante transformaciones aleatorias (rotaciones, giros, zoom, traslaciones, etc.).\n","Esto amplía el conjunto de entrenamiento sin necesidad de recopilar más datos reales, lo que mejora la generalización del modelo.\n","\n","```python\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.1,\n","    horizontal_flip=True\n",")\n","\n","datagen.fit(x_train)\n","```\n","\n","Cuándo usarlo: Cuando el conjunto de imágenes es pequeño o el modelo sobreajusta rápidamente (alta precisión en entrenamiento pero baja en validación).\n","\n","\n","<b> b) Dropout en capas convolucionales </b>\n","\n","El Dropout desactiva aleatoriamente un porcentaje de neuronas durante el entrenamiento, lo que evita la dependencia excesiva de ciertas activaciones y mejora la robustez del modelo.\n","\n","```python\n","from tensorflow.keras import layers, models\n","\n","model = models.Sequential([\n","    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Dropout(0.25),  # desactiva el 25% de las neuronas\n","    layers.Conv2D(64, (3,3), activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Dropout(0.25), # desactiva el 25% de las neuronas\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.5), # desactiva el 25% de las neuronas\n","    layers.Dense(10, activation='softmax')\n","])\n","```\n","Cuándo usarlo: Cuando el modelo muestra alta precisión en el entrenamiento y baja precisión en validación.\n","\n","<b> c) Batch Normalization </b>\n","\n","La Batch Normalization normaliza las activaciones intermedias en cada mini-lote, estabilizando el entrenamiento y acelerando la convergencia.\n","En las CNN, suele colocarse después de las capas convolucionales y antes de la función de activación.\n","\n","```python\n","model = models.Sequential([\n","    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n","    layers.BatchNormalization(),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Conv2D(64, (3,3), activation='relu'),\n","    layers.BatchNormalization(),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dense(10, activation='softmax')\n","])\n","```\n","Cuándo usarlo: Cuando el entrenamiento es inestable, la pérdida oscila entre épocas o la red tarda en converger.\n","\n","<b> d) Early Stopping </b>\n","\n","El Early Stopping detiene automáticamente el entrenamiento cuando el rendimiento en validación deja de mejorar, evitando el sobreentrenamiento y reduciendo el tiempo total de entrenamiento.\n","\n","```python\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stop = EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=50,\n","    batch_size=32,\n","    validation_split=0.2,\n","    callbacks=[early_stop]\n",")\n","```\n","\n","Parámetros principales:\n","\n","* `monitor`: métrica a observar (por ejemplo, val_loss o val_accuracy).\n","\n","* `patience`: número de épocas sin mejora antes de detener.\n","\n","* `restore_best_weights`: recupera los mejores pesos previos al sobreajuste.\n","\n","Cuándo usarlo: Siempre que el entrenamiento prolongado empiece a degradar el rendimiento de validación.\n","\n","<b> e) Regularización L2 en capas convolucionales </b>\n","\n","La regularización L2 penaliza los pesos grandes, fomentando una distribución más equilibrada de las activaciones.\n","En CNN se aplica frecuentemente a las capas convolucionales y densas.\n","\n","```python\n","from tensorflow.keras import regularizers\n","\n","model = models.Sequential([\n","    layers.Conv2D(64, (3,3), activation='relu',\n","                  kernel_regularizer=regularizers.l2(0.001),\n","                  input_shape=(64,64,3)),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu',\n","                 kernel_regularizer=regularizers.l2(0.001)),\n","    layers.Dense(10, activation='softmax')\n","])\n","```\n","Cuándo usarlo: Cuando la red tiene muchas capas o parámetros y se observa sobreajuste."],"metadata":{"id":"tG4qmst0ZaRH"}},{"cell_type":"markdown","source":["### <b>2.5.2. Ajuste de Hiperparámetros (Hyperparameter Tunning)</b>\n","\n","Los hiperparámetros en una CNN definen la estructura y el comportamiento de la red, y su correcta selección puede marcar una gran diferencia en el rendimiento final.\n","\n","Ejemplos comunes:\n","\n","* Tamaño del kernel (3×3, 5×5)\n","\n","* Número de filtros por capa\n","\n","* Tipo de pooling (Max o Average)\n","\n","* Tasa de Dropout\n","\n","* Tasa de aprendizaje (learning rate)"],"metadata":{"id":"ia0226Ubb33y"}},{"cell_type":"markdown","source":["### <b> 3.5.3 Generación de Embeddings Visuales (Feature Extraction)</b>\n","\n","En las redes convolucionales, el Embedding visual es el vector de características obtenido tras procesar los píxeles.\n","\n","<b>¿Qué problema soluciona?</b>\n","\n","* <b>Invariabilidad y Rigidez:</b> Una CNN básica solo clasifica (\"Perro\" o \"Gato\"). El Embedding soluciona esto permitiendo representar la imagen como un \"código numérico\". Esto sirve para comparar imágenes nuevas que el modelo no ha visto (ej. reconocimiento facial) midiendo la distancia entre sus vectores.\n","\n","* <b>Overfitting por exceso de parámetros:</b> Al aplanar (`Flatten`) capas convolucionales grandes, se crean millones de conexiones que memorizan ruido. El Embedding, junto con el pooling global, soluciona esto comprimiendo la información en un resumen denso y pequeño.\n","\n","<b>Parámetros de la Capa y su Configuración</b>\n","\n","Para definir este \"Embedding Visual\", jugamos con la estructura final de la red antes de la salida:\n","\n","1. GlobalAveragePooling2D (GAP):\n","\n","    * Para qué sirve: Es el sustituto de mejora del Flatten. Reduce cada mapa de características a un solo valor (el promedio).\n","\n","    * Impacto: Elimina la mayor parte de los parámetros de la red, forzándola a generalizar.\n","\n","2. Dimensiones del Vector (Capa Dense):\n","\n","    * Para qué sirve: Define el tamaño del vector final (cuántas neuronas resumen la imagen).\n","\n","    * Cómo se define: Es un hiperparámetro. Valores típicos son 128 o 256.\n","\n","    * Regla: Suficiente para diferenciar objetos, pero lo bastante pequeño para no memorizar la imagen completa.\n","\n","<b>Implementación y Situación en la Estructura CNN</b>\n","\n","A diferencia de las ANN, en las CNN el embedding se sitúa al final de la rama convolucional, actuando como el \"cuello de botella\" de conocimiento antes de la decisión final.\n","\n","```python\n","from tensorflow.keras import layers, models\n","\n","# Parámetros de la fase de mejora\n","dim_embedding = 128  # Tamaño del vector que resume la imagen\n","\n","model = models.Sequential([\n","    # Fase Convolucional (Extracción de patrones)\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","\n","    # --- FASE DE MEJORA: CONSTRUCCIÓN DEL EMBEDDING ---\n","    # Componente 1: Reducción de dimensionalidad espacial (GAP)\n","    layers.GlobalAveragePooling2D(),\n","    \n","    # Componente 2: Creación del vector denso (Embedding Visual)\n","    layers.Dense(dim_embedding,name=\"visual_embedding\"),\n","    layers.BatchNormalization(), # <--- Garantiza estabilidad y comparabilidad\n","    \n","    # --- SALIDA ---\n","    layers.Dropout(0.5), # Mejora adicional\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","```"],"metadata":{"id":"GPufXgyOi-Hh"}},{"cell_type":"markdown","source":["## 2.6. Fase de producción"],"metadata":{"id":"IVxiKFHJ1GEm"}},{"cell_type":"markdown","source":["# <b>3. Recurrent Nerual Network (RNN) model in keras </b>\n","\n","Una Red Neuronal Recurrente (RNN) es un tipo de red neuronal diseñada para trabajar con datos secuenciales o series temporales. A diferencia de las DNN y CNN, las RNN tienen \"memoria\", ya que utilizan información de pasos anteriores para influir en la salida actual mediante conexiones cíclicas. Se utilizan principalmente en tareas de <b>procesamiento de lenguaje natural (NLP), predicción de series de tiempo y reconocimiento de voz</b>.\n","\n","#### <b>Estructura básica de una red neuronal recurrente</b>\n","\n","Una arquitectura de red neuronal recurrente consta de las siguientes partes principales:\n","\n","* Capa de entrada (Input Layer): Recibe secuencias de datos con dimensiones (pasos de tiempo, características).\n","\n","* Capas Recurrentes (RNN/LSTM/GRU): Procesan la secuencia paso a paso, manteniendo un estado oculto que actúa como memoria.\n","\n","* Capas Densas (Fully Connected): Procesan la información extraída de la secuencia para realizar la tarea final.\n","\n","* Capa de salida (Output Layer): Genera la predicción final (un valor, una categoría o una secuencia)."],"metadata":{"id":"EDyVmaS_Z200"}},{"cell_type":"markdown","source":["## Librerías necesarias\n","\n","Además de las librerías base, para las RNN solemos importar capas específicas de Keras diseñadas para el manejo de secuencias.\n","\n","```python\n","# Capas específicas para redes recurrentes\n","from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Embedding\n","```"],"metadata":{"id":"1Nsj6VWp1X--"}},{"cell_type":"markdown","source":["## 3.1 Fase de Construcción\n","\n","En esta fase se define la arquitectura de la red, seleccionando el tipo de celda recurrente adecuada según la complejidad de la secuencia y la memoria necesaria."],"metadata":{"id":"tKUgO-y11mJ6"}},{"cell_type":"markdown","source":["### 3.1.1. Capa de entrada (Input Layer)\n","\n","Las RNN requieren que los datos tengan una forma tridimensional al llegar a la capa recurrente: (`batch_size`, `timesteps`, `features`). La configuración de esta entrada depende de si necesitamos traducir los datos antes de procesarlos.\n","\n","#### <b>A. Para Datos Numéricos (Sin Embedding)</b>\n","\n","Se usa cuando los datos ya son números reales (ej. precios, sensores).\n","\n","* `timesteps`: Longitud de la secuencia (cuántos pasos atrás miramos).\n","\n","* `features`: Número de variables en cada paso de tiempo.\n","\n","<b>API Funcional </b>\n","\n","```python\n","inputs = layers.Input(shape=(timesteps, features), name='input_layer')\n","```\n","\n","<b>API Secuencial </b>\n","\n","```python\n","model = models.Sequential()\n","model.add(layers.Input(shape=(timesteps, features), name='input_layer'))\n","```\n","\n","#### <b>B. Para Datos Categóricos (Con Capa Embedding)</b>\n","\n","Se usa cuando los datos son IDs o palabras. El Embedding se convierte en la \"capa de entrada\" de facto porque transforma enteros en vectores.\n","\n","* `input_dim`: Tamaño del vocabulario (ej. 5000 palabras o 500 jugadores).\n","\n","* `output_dim`: Equivale al número de features que recibirá la RNN (el tamaño del vector).\n","\n","* `input_length`: Equivale a los timesteps (longitud de la secuencia).\n","\n","<b>API Funcional </b>\n","\n","```python\n","# Definimos la entrada de IDs (secuencia de 20 números enteros)\n","inputs = layers.Input(shape=(20,), dtype='int32', name='input_ids')\n","# El embedding se aplica sobre esa entrada\n","embedding = layers.Embedding(input_dim=1000, output_dim=64)(inputs)\n","```\n","\n","<b>API Secuencial </b>\n","\n","```python\n","model = models.Sequential()\n","# El embedding recibe una secuencia de enteros y entrega una de vectores\n","model.add(layers.Embedding(input_dim=1000, output_dim=64, input_length=20, name='embedding_input'))\n","```"],"metadata":{"id":"o8bpxJX41qhR"}},{"cell_type":"markdown","source":["### 3.1.2. Capas Recurrentes (Hidden Layers)\n","\n","Son el núcleo de la red. En Keras existen tres tipos principales:\n","\n","<b>a) SimpleRNN</b>\n","\n","Es la forma básica, pero sufre de gradiente desvanecido, lo que limita su memoria a corto plazo.\n","\n","* <b>Activación:(`activation`)</b> Se usa mayoritariamente `tanh` (tangente hiperbólica) para mantener los valores entre -1 y 1, evitando que los valores exploten en secuencias largas. También puede usarse `relu`, pero requiere cuidado con la inicialización de pesos.\n","\n","* <b>Cuándo usarla:</b> Para problemas de memoria muy corta y baja complejidad.\n","\n","* <b>Caso de uso real</b>: Predicción de patrones cíclicos simples, como la vibración de una máquina en los últimos segundos para detectar fallos inmediatos.\n","\n","<b>API Funcional</b>\n","\n","```python\n","# return_sequences=True: La capa devuelve la secuencia completa (3D)\n","# Es necesario para conectar con otra capa recurrente (en este caso otra SimpleRNN)\n","x = layers.SimpleRNN(units=64,\n","                     activation='tanh',\n","                     return_sequences=True)(inputs)\n","\n","# return_sequences=False: La capa devuelve solo el último estado (2D)\n","# Se usa para finalizar el bloque recurrente y pasar a capas Densas\n","x = layers.SimpleRNN(units=32,\n","                     activation='tanh',\n","                     return_sequences=False)(x)\n","```\n","\n","<b>API Secuencial</b>\n","\n","```python\n","# Capa 1: Mantiene la dimensión temporal (3D) para que la siguiente capa pueda procesar la secuencia\n","model.add(layers.SimpleRNN(units=64,\n","                           activation='tanh',\n","                           return_sequences=True))\n","\n","# Capa 2: Colapsa la secuencia en un único vector (2D)\n","# Prepara los datos para la interpretación final en las capas Dense\n","model.add(layers.SimpleRNN(units=32,\n","                           activation='tanh',\n","                           return_sequences=False))\n","```\n","\n","<b>b) LSTM (Long Short-Term Memory) </b>\n","\n","Soluciona el problema del \"gradiente desvanecido\" mediante puertas de control de información.\n","\n","* <b>Activation de estado</b> (`activation`): Utiliza mayoritariamente `tanh` (tangente hiperbólica). Se encarga de transformar la información que sale de la celda para que los valores se mantengan en un rango de [-1, 1], evitando que los datos crezcan de forma descontrolada tras muchos pasos de tiempo\n","* <b>Activación interna: (`recurrent_activation`)</b> Utiliza `sigmoid` para las puertas (gates), ya que necesita valores entre 0 (olvidar) y 1 (recordar).\n","\n","* <b>Cuándo usarla</b>: Para secuencias largas donde el contexto inicial es vital para el final.\n","\n","* <b> Caso de uso real</b>: Generación de subtítulos automáticos o traducción de textos, donde una palabra al principio de la frase define el género de una palabra al final.\n","\n","<b>API Funcional</b>\n","\n","```python\n","# Capa 1: DEBE ser True porque conectamos con otra capa recurrente.\n","# Salida: Tensor 3D (batch, timesteps, units)\n","x = layers.LSTM(128,\n","                activation='tanh',\n","                recurrent_activation='sigmoid',\n","                return_sequences=True)(inputs)\n","\n","# Capa 2: Usamos False porque lo siguiente es una capa Densa (Dense).\n","# La capa Densa no entiende de \"tiempo\", solo de vectores.\n","# Salida: Tensor 2D (batch, units) -> Solo el \"resumen\" final de la secuencia\n","x = layers.LSTM(64,\n","                activation='tanh',\n","                recurrent_activation='sigmoid',\n","                return_sequences=False)(x)\n","```\n","\n","<b>API Secuencial</b>\n","\n","```python\n","odel = models.Sequential()\n","\n","# Primera capa recurrente: Siempre return_sequences=True si hay más RNNs debajo.\n","# Esto mantiene la estructura de \"secuencia\" para la siguiente capa.\n","model.add(layers.LSTM(128,\n","                      activation='tanh',\n","                      return_sequences=True,\n","                      input_shape=(timesteps, features)))\n","\n","# Última capa recurrente: return_sequences=False (valor por defecto).\n","# Esto \"aplana\" la salida temporal para que la capa Dense pueda procesarla.\n","model.add(layers.LSTM(64,\n","                      activation='tanh',\n","                      return_sequences=False))\n","\n","```\n","\n","<b>c) GRU (Gated Recurrent Unit)</b>\n","\n","Es una variante simplificada de la LSTM que combina las puertas de olvido y entrada en una sola \"puerta de actualización\". Esto reduce el número de parámetros, haciendo que el modelo sea más rápido de entrenar y menos propenso al overfitting en datasets pequeños.\n","\n","* <b>Activación de estado</b> (`activation`): Utiliza tanh para calcular el nuevo contenido de la memoria, manteniendo los valores en el rango [-1, 1].\n","\n","* <b>Activación interna</b> (`recurrent_activation`): Utiliza sigmoid para las puertas de actualización (update) y reseteo (reset), determinando cuánta información del pasado debe pasar al futuro.\n","\n","* <b>Cuándo usarla</b>: Cuando se busca eficiencia computacional, se tienen datasets limitados o se requiere una respuesta rápida en tiempo real.\n","\n","* <b>Caso de uso real</b>: Análisis de sentimiento en Twitter o respuestas rápidas en chatbots, donde la brevedad del texto permite usar una arquitectura más ligera que la LSTM.\n","\n","<b>API Funcional</b>\n","\n","```python\n","# return_sequences=True: Mantiene la dimensión temporal para apilar otra RNN\n","# Se especifican ambas activaciones para control total del flujo\n","x = layers.GRU(64,\n","               activation='tanh',\n","               recurrent_activation='sigmoid',\n","               return_sequences=True)(inputs)\n","\n","# return_sequences=False: Colapsa la secuencia en un único vector para la capa Dense\n","x = layers.GRU(32,\n","               activation='tanh',\n","               recurrent_activation='sigmoid',\n","               return_sequences=False)(x)\n","```\n","\n","<b>API Secuencial</b>\n","\n","```python\n","# Primera capa: Mantiene la secuencia (3D) para la siguiente capa GRU\n","model.add(layers.GRU(64,\n","                     activation='tanh',\n","                     recurrent_activation='sigmoid',\n","                     return_sequences=True))\n","\n","# Segunda capa: Entrega solo el último estado (2D) para pasar a la fase de interpretación\n","model.add(layers.GRU(32,\n","                     activation='tanh',\n","                     recurrent_activation='sigmoid',\n","                     return_sequences=False))\n","```"],"metadata":{"id":"k_AJh-yl2MyC"}},{"cell_type":"markdown","source":["### 3.1.3. Capas Densas (Interpretation Layers)\n","\n","Tras la recurrencia, estas capas \"toman decisiones\" basadas en la memoria extraída. Aquí las funciones de activación cambian según la profundidad:\n","\n","* <b>ReLU (`activation='relu'`):</b> La más común en capas intermedias para evitar el desvanecimiento del gradiente en la parte densa.\n","\n","* <b>Leaky ReLU:</b> Útil si se detectan muchas \"neuronas muertas\" durante el entrenamiento.\n","\n","* <b>Dropout:</b> No es una activación, pero es vital tras una densa para evitar que la red dependa de neuronas específicas.\n","\n","<b>API Funcional</b>\n","\n","```python\n","x = layers.Dense(32, activation='relu')(x)\n","x = layers.Dropout(0.2)(x)\n","```\n","\n","<b>API Secuencial</b>\n","\n","```python\n","model.add(layers.Dense(32, activation='relu'))\n","model.add(layers.Dropout(0.2))\n","```"],"metadata":{"id":"0vAOY7sW5aUX"}},{"cell_type":"markdown","source":["### 3.1.4. Capa de salida (Output Layer)\n","\n","La función de activación aquí define la naturaleza del problema:\n","\n","<b>1. Regresión (Predicción de un valor numérico):</b>\n","\n","* Activación: `linear` (o ninguna). Permite cualquier rango de salida.\n","\n","* Ejemplo: Predecir el precio del Bitcoin <b>mañana</b>.\n","\n","<b>2. Clasificación Binaria (Dos clases, 0 o 1):</b>\n","\n","* Activación: `sigmoid`. Devuelve una probabilidad entre 0 y 1.\n","\n","* Ejemplo: ¿Es este audio voz humana o ruido?\n","\n","<b>3. Clasificación Multiclase (Más de dos clases):</b>\n","\n","* Activación: `softmax`. Devuelve una distribución de probabilidad que suma 1 entre todas las clases.\n","\n","* Ejemplo: Clasificar un texto en \"Positivo\", \"Neutro\" o \"Negativo\".\n","\n","<b>API Funcional</b>\n","\n","```python\n","# Ejemplo para clasificación multiclase con 3 categorías\n","outputs = layers.Dense(3, activation='softmax', name='output_layer')(x)\n","model = models.Model(inputs=inputs, outputs=outputs)\n","```\n","\n","<b>API Secuencial</b>\n","\n","```python\n","# Ejemplo para clasificación multiclase con 3 categorías\n","model.add(layers.Dense(3, activation='softmax', name='output_layer'))\n","```"],"metadata":{"id":"OmSoEc-45vS5"}},{"cell_type":"markdown","source":["### 3.1.5. Construcción completa\n","\n","En esta fase definimos la estructura de la red. Para las RNN, es fundamental asegurar que los tensores pasen de 3D a 2D antes de llegar a las capas de interpretación final.\n","\n","<b>API Funcional\n","\n","```python\n","from tensorflow.keras import models, layers\n","\n","# 1. Definición de la Entrada\n","inputs = layers.Input(shape=(10, 1), name='input_layer')\n","\n","# 2. Bloque Recurrente\n","# Mantenemos la secuencia (return_sequences=True) para conectar con la siguiente capa\n","x = layers.LSTM(64,\n","                activation='tanh',\n","                recurrent_activation='sigmoid',\n","                return_sequences=True,\n","                name='lstm_layer')(inputs)\n","\n","# Finalizamos la recurrencia (return_sequences=False) para obtener un vector resumen\n","x = layers.GRU(32,\n","               activation='tanh',\n","               recurrent_activation='sigmoid',\n","               return_sequences=False,\n","               name='gru_layer')(x)\n","\n","# 3. Capa de Interpretación\n","x = layers.Dense(16, activation='relu', name='dense_interpretation')(x)\n","x = layers.Dropout(0.2)(x)\n","\n","# 4. Capa de Salida (Ejemplo: Clasificación de 3 categorías)\n","outputs = layers.Dense(3, activation='softmax', name='output_layer')(x)\n","\n","# Creamos el modelo vinculando entrada y salida\n","model_func = models.Model(inputs=inputs, outputs=outputs, name=\"RNN_Functional_Model\")\n","\n","model_func.summary()\n","```\n","\n","<b>API Secuencial\n","\n","```python\n","from tensorflow.keras import models, layers\n","\n","# Inicializamos el modelo\n","model = models.Sequential(name=\"RNN_Sequential_Model\")\n","\n","# 1. Capa de entrada: Obligatorio formato 3D (timesteps, features)\n","model.add(layers.Input(shape=(10, 1)))\n","\n","# 2. Bloque Recurrente: Apilamos dos capas\n","# Capa intermedia: return_sequences=True para pasar la secuencia a la siguiente\n","model.add(layers.LSTM(64, activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n","\n","# Capa final recurrente: return_sequences=False para entregar un vector 2D a la parte Densa\n","model.add(layers.GRU(32, activation='tanh', recurrent_activation='sigmoid', return_sequences=False))\n","\n","# 3. Capa de Interpretación (Densa)\n","model.add(layers.Dense(16, activation='relu'))\n","model.add(layers.Dropout(0.2))\n","\n","# 4. Capa de Salida (Ejemplo: Clasificación de 3 categorías)\n","model.add(layers.Dense(3, activation='softmax'))\n","\n","model.summary()\n","```"],"metadata":{"id":"2MQ48P41Lf7a"}},{"cell_type":"markdown","source":["## 3.2. Fase de Compilación\n","\n","\n","Configuramos cómo aprenderá la red. En RNN, el optimizador `RMSprop` suele recomendarse para datos secuenciales ruidosos, aunque `Adam` sigue siendo la opción general más robusta.\n","\n"],"metadata":{"id":"Ma0wP_6D5p5K"}},{"cell_type":"markdown","source":["<b>Optimizador (optimizer)</b>\n","\n","El optimizador actualiza los pesos de la red para minimizar la pérdida.\n","\n","* `MSprop`: Históricamente muy recomendado para RNNs, ya que gestiona muy bien los gradientes que pueden variar drásticamente en secuencias.\n","\n","* `Adam`: La opción más robusta y utilizada por defecto; suele converger más rápido.\n","\n","<b>Función de Pérdida (loss)</b>\n","\n","Define cómo medimos el error del modelo. Debe ir en consonancia con la capa de salida:\n","\n","* `mean_squared_error` (MSE): Para problemas de regresión (ej. predecir el valor de una acción).\n","\n","* `binary_crossentropy`: Para clasificación binaria (ej. ¿Es este audio un fraude? Sí/No).\n","\n","* `categorical_crossentropy`: Para clasificación multiclase (cuando las etiquetas están en formato One-Hot).\n","\n","* `sparse_categorical_crossentropy`: Para clasificación multiclase (cuando las etiquetas son números enteros).\n","\n","<b>Métricas (metrics)</b>\n","\n","Se utilizan para monitorizar el rendimiento de forma humana:\n","\n","* `accuracy`: Porcentaje de aciertos (para clasificación).\n","\n","* `mae` (Mean Absolute Error): Error medio en las unidades de la variable (para regresión).\n","\n","Caso A: Regresión (Series Temporales)\n","\n","```python\n","# API Funcional/Secuencial (el método compile es idéntico)\n","model.compile(\n","    optimizer='adam',\n","    loss='mean_squared_error',\n","    metrics=['mae']\n",")\n","```\n","\n","Caso B: Clasificación Binaria\n","\n","```python\n","model.compile(\n","    optimizer='rmsprop',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","```\n","\n","Caso C: Clasificación Multiclase\n","\n","```python\n","model.compile(\n","    optimizer='adam',\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","```"],"metadata":{"id":"qzWEZyy-1kNC"}},{"cell_type":"markdown","source":["## 3.3. Fase de Entrenamiento\n","\n","\n","Una vez compilado el modelo, se procede a entrenarlo.\n","\n","Durante esta fase, el modelo ajusta sus pesos internos para minimizar la función de pérdida definida en la compilación y mejorar las métricas, teniendo en cuenta la dependencia temporal de los datos.\n"],"metadata":{"id":"yN8AYD6i4tJt"}},{"cell_type":"markdown","source":["El entrenamiento se realiza con el método `fit()`, que recibe los datos (en formato 3D para RNN), el número de épocas, el tamaño de los lotes y, opcionalmente, un conjunto de validación.\n","\n","<b>Parámetros del método</b>\n","\n","* <b>x, y</b>: Datos de entrada (tensores 3D: [samples, timesteps, features]) y etiquetas verdaderas.\n","\n","* <b>epochs</b>: Número de pasadas completas sobre el conjunto de entrenamiento.\n","\n","* <b>batch_size</b>: Tamaño del lote. En RNNs, un lote más grande suele dar estabilidad, pero consume más memoria.\n","\n","* <b>validation_data</b>: Tupla (x_val, y_val) para evaluar al final de cada época.\n","\n","* <b>validation_split</b>: Fracción de x/y usada automáticamente como validación (si no se pasa validation_data).\n","\n","* <b>verbose</b>: Nivel de detalle (0: silencioso, 1: barra de progreso, 2: por época).\n","\n","* <b>shuffle</b>: (Importante en RNN) Por defecto es True. En series temporales muy específicas, a veces se desactiva si se usa stateful=True.\n","\n","<b>Entrenamiento (clasificación – RNN con secuencias de texto/audio)</b>\n","\n","```python\n","history = model.fit(\n","    x_train, y_train,                 # Datos de entrenamiento (3D tensor)\n","    epochs=20,                         # Número de épocas\n","    batch_size=64,                     # Tamaño del lote\n","    validation_data=(x_val, y_val),    # Conjunto de validación\n","    verbose=1                          # Mostrar progreso\n",")\n","```\n","* Uso típico con salida `softmax` y pérdida `categorical_crossentropy` (ej. análisis de sentimiento o clasificación de señales).\n","\n","* `history` almacena la evolución de loss y métricas por época.\n","\n","<b>Entrenamiento (regresión – predicción de series temporales)</b>\n","\n","```python\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=50,\n","    batch_size=32,\n","    validation_split=0.2,  # Reserva automáticamente el 20% para validación\n","    verbose=1\n",")\n","```\n","\n","* Uso típico con salida `linear` y pérdidas de regresión (`mse` o `mae`).\n","\n","* En series temporales, el `validation_split` toma el último 20% de los datos, lo cual es correcto para no \"predecir el pasado con datos del futuro\".\n"],"metadata":{"id":"MB11YmEeFojU"}},{"cell_type":"markdown","source":["## 3.4 Fase de Evaluación\n","\n","Una vez finalizado el entrenamiento, es fundamental evaluar el modelo con datos que no han sido utilizados previamente (conjunto de test). En las RNN, esta fase permite verificar si el modelo ha aprendido realmente las dependencias temporales y la estructura de la secuencia, o si simplemente está sufriendo de overfitting (memorización)."],"metadata":{"id":"EcRgu0urFtQc"}},{"cell_type":"markdown","source":["El método principal es `evaluate()`, que calcula la pérdida y las métricas finales sobre el conjunto de prueba.\n","\n","<b>Parámetros del método </b>\n","\n","* <b>x, y</b>: Datos de prueba y etiquetas reales (deben tener el mismo preprocesamiento que los de entrenamiento).\n","\n","* <b>batch_size</b>: Tamaño del lote para la evaluación (suele ser el mismo que en el entrenamiento).\n","\n","* <b>verbose</b>: Nivel de detalle (0 o 1).\n","\n","<b>Evaluación (clasificación – RNN)</b>\n","\n","```python\n","# Evaluación del rendimiento global en clasificación\n","loss, accuracy = model.evaluate(\n","    x_test, y_test,             # Datos de test\n","    verbose=1                   # Mostrar progreso\n",")\n","\n","print(f\"Pérdida en Test: {loss:.4f}\")\n","print(f\"Precisión en Test: {accuracy:.4f}\")\n","```\n","\n","* En clasificación, la métrica clave suele ser la precisión (accuracy), que indica el porcentaje de secuencias correctamente etiquetadas.\n","\n","<b>Evaluación (regresión – predicción continua)</b>\n","\n","```python\n","# Evaluación del error en predicción de series temporales\n","results = model.evaluate(\n","    x_test, y_test,\n","    verbose=1\n",")\n","\n","print(f\"Error Cuadrático Medio (MSE): {results[0]:.4f}\")\n","print(f\"Error Absoluto Medio (MAE): {results[1]:.4f}\")\n","```\n","\n","* En regresión de series temporales, el `MAE` es especialmente útil porque nos indica cuánto se desvía nuestra predicción, en promedio, de la unidad de medida real (ej. cuántos euros de diferencia hay en la predicción del precio).\n","\n","<b>Interpretación de resultados</b>\n","\n","* Pérdida (Loss): Indica qué tan lejos están las predicciones de los valores reales. Cuanto menor sea, mejor.\n","\n","* Generalización: Si existe una diferencia muy grande entre los resultados de entrenamiento y los de test, el modelo tiene overfitting. En RNNs, esto es común si la red es demasiado compleja para la longitud de la secuencia proporcionada."],"metadata":{"id":"YOfihw4rF599"}},{"cell_type":"markdown","source":["## 3.5. Fase de Mejora\n","\n","Una vez definida la estructura base, aplicamos técnicas avanzadas para estabilizar el gradiente y mejorar la generalización del modelo en datos nuevos.\n"],"metadata":{"id":"zOYuKppLGnKX"}},{"cell_type":"markdown","source":["### 3.5.1. Regularización Recurrente (Deep Memory Dropout)\n","\n","A diferencia del Dropout en capas densas, aquí actuamos sobre la memoria de la secuencia. Usamos parámetros internos de las capas `LSTM` o `GRU`:\n","\n","* `dropout`: Regulariza las conexiones de los datos de entrada.\n","\n","* `recurrent_dropout`: Regulariza el paso de información de un instante de tiempo al siguiente. Es vital para que la red no dependa de un único camino de memoria.\n","\n","```python\n","# Mejora de la capa recurrente con doble Dropout interno\n","model.add(layers.LSTM(64,\n","                      dropout=0.2,\n","                      recurrent_dropout=0.2,\n","                      return_sequences=True))\n","```\n","\n","* <b>Para qué sirve</b>: Evita que la red se \"aprenda de memoria\" la secuencia de entrenamiento (overfitting). A diferencia del dropout normal, este protege la memoria a largo plazo de la celda.\n","\n","* <b>Ejemplo</b>: Estás entrenando un modelo para escribir textos al estilo de Cervantes. Sin esta mejora, el modelo solo repetirá frases exactas de \"El Quijote\". Con ella, aprenderá la estructura del lenguaje y podrá crear frases nuevas."],"metadata":{"id":"Y6OMwZ_hnmpe"}},{"cell_type":"markdown","source":["### 3.5.2. Capas Bidireccionales (Bidirectional)\n","\n","Esta técnica permite que la red procese la secuencia en ambos sentidos (pasado $\\rightarrow$ futuro y futuro $\\rightarrow$ pasado). Dobla la capacidad de la red al permitirle conocer el contexto posterior antes de decidir sobre un punto actual.\n","\n","\n","```python\n","# Envolvemos la capa recurrente para ver la secuencia en ambas direcciones\n","model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=False)))\n","```\n","\n","* Se puede aplicar a cualquier capa reccurente de Keras (`SipleRNN`,`LSTM` o `GRU`)\n","\n","* <b>Para qué sirve</b>: Permite que la red entienda el contexto global (pasado y futuro). Es vital cuando el sentido de una parte de la secuencia depende de lo que viene después.\n","\n","* <b>Ejemplo</b>: En la frase \"El banco estaba lleno de gente esperando a cobrar\", la red no sabe si \"banco\" es un mueble o una entidad financiera hasta que lee el final de la frase. La capa bidireccional permite \"mirar hacia adelante\" para entender el principio.\n"],"metadata":{"id":"uU32AsL-n4Yp"}},{"cell_type":"markdown","source":["### 3.5.3. Gradient Clipping (Recorte de Gradientes)\n","\n","Las RNN suelen tener problemas de Gradientes Explosivos (los pesos crecen tanto que el modelo da error `NaN`). El clipping limita el valor del gradiente durante la actualización para mantener la estabilidad.\n","\n","```python\n","from tensorflow.keras import optimizers\n","\n","# Se aplica en el optimizador: el gradiente no superará el valor de 1.0\n","optimizador_estable = optimizers.Adam(learning_rate=0.001, clipvalue=1.0)\n","\n","model.compile(optimizer=optimizador_estable, loss='mse')\n","```\n","\n","* <b>Para qué sirve</b>: Evita el problema del Gradiente Explosivo. En las RNN, al multiplicar muchos valores a lo largo del tiempo, el error puede crecer tanto que se vuelve infinito (`NaN`). El clipping pone un \"techo\" a ese error.\n","\n","* <b>Ejemplo</b>: Durante el entrenamiento, notas que de repente la pérdida (loss) pasa de 0.5 a NaN y el modelo deja de aprender. Aplicar clipping estabiliza el aprendizaje para que el modelo no se \"rompa\"."],"metadata":{"id":"DE7KjMkpo9NS"}},{"cell_type":"markdown","source":["### 3.5.4. Callbacks: Early Stopping y ReduceLROnPlateau\n","\n","Los callbacks son \"automatismos\" que vigilan el entrenamiento:\n","\n","* <b>EarlyStopping</b>: Detiene el entrenamiento si el error de validación deja de mejorar, evitando el sobreajuste.\n","\n","* <b>ReduceLROnPlateau</b>: Reduce el learning_rate si el modelo se estanca, permitiendo un ajuste más fino.\n","\n","```python\n","from tensorflow.keras import callbacks\n","\n","mejora_callbacks = [\n","    callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n","    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)\n","]\n","\n","# Se integran en el entrenamiento\n","model.fit(X_train, y_train, epochs=100, callbacks=mejora_callbacks, validation_split=0.2)\n","```\n","\n","#### Early Stopping (Parada Temprana)\n","\n","* <b>Para qué sirve</b>: Detiene el entrenamiento automáticamente en el momento exacto antes de que el modelo empiece a memorizar ruido. Ahorra tiempo y energía.\n","\n","* <b>Ejemplo</b>: Configuras 100 épocas, pero en la época 20 el error de validación deja de bajar. El EarlyStopping corta el proceso ahí, evitando que las otras 80 épocas estropeen el modelo por sobreajuste.\n","\n","#### ReduceLROnPlateau (Reducción de Tasa de Aprendizaje)\n","\n","* <b>Para qué sirve</b>: Reduce la velocidad de aprendizaje cuando el modelo se estanca. Es como si el modelo fuera \"frenando\" al acercarse a la solución óptima para no pasarse de largo.\n","\n","* <b>Ejemplo</b>: Tu modelo llega a una precisión del 85% y no sube más. Al reducir el learning_rate, el modelo empieza a dar pasos más pequeños y cuidadosos, logrando llegar quizás al 90%."],"metadata":{"id":"cb0v34YwpVfJ"}}]}